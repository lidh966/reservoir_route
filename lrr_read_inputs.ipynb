{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Water_Supply_Drought'\n",
    "data_dir = f'{base_dir}/data'\n",
    "\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Grids & Sort by Upstream-Downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read geo files related to a given HUC4 basin ---- #\n",
    "\n",
    "huc4 = '0601'\n",
    "\n",
    "nhd_data_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Drought_Cycle_Analysis/Data'\n",
    "crs = 'EPSG:4326'\n",
    "huc2_conus = [f'0{i}' if i<10 else f'{i}' for i in range(1, 19)]\n",
    "\n",
    "# read HUCs\n",
    "huc2 = huc4[0:2]\n",
    "gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "gdf_huc2_all = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "gdf_huc4_all = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "gdf_huc6_all = gpd.read_file(gdb_file, layer='WBDHU6')\n",
    "gdf_huc8_all = gpd.read_file(gdb_file, layer='WBDHU8')\n",
    "gdf_huc10_all = gpd.read_file(gdb_file, layer='WBDHU10')\n",
    "\n",
    "# set crs\n",
    "gdf_huc2_all = gdf_huc2_all.set_crs(crs, inplace=False, allow_override=True)    # includes the huc2 region\n",
    "gdf_huc4_all = gdf_huc4_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc4 subregions in this huc2 region\n",
    "gdf_huc6_all = gdf_huc6_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc6 basins in this huc2 region\n",
    "gdf_huc8_all = gdf_huc8_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc8 subbasins in this huc2 region\n",
    "gdf_huc10_all = gdf_huc10_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc10 subbasins in this huc2 region\n",
    "\n",
    "########## Prepare flow lines ##########\n",
    "\n",
    "if huc2 == '03':    # multiple NHDP files for 03\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['N','S','W']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['N','S','W']]\n",
    "elif huc2 == '10': \n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['U','L']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['U','L']]\n",
    "else:\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDPlusAttributes']\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDSnapshot/Hydrography']\n",
    "\n",
    "gdf_flow_list = []\n",
    "for flow_attr_file, hydro_file in zip(flow_attr_file_list, hydro_file_list):\n",
    "    gdf_fline_vaa = gpd.read_file(flow_attr_file, layer='PlusFlowlineVAA')\n",
    "    gdf_fline = gpd.read_file(hydro_file, layer='NHDFlowline')\n",
    "\n",
    "    # change COMID to ComID if the error exists\n",
    "    if not 'ComID' in gdf_fline:\n",
    "        gdf_fline.rename(columns={'COMID':'ComID'}, inplace=True)\n",
    "\n",
    "    # change vaa file ComID to int\n",
    "    to_int_var = ['ComID', 'StreamOrde', 'StreamCalc']\n",
    "    gdf_fline_vaa[to_int_var] = gdf_fline_vaa[to_int_var].astype(int)\n",
    "\n",
    "    # merge this two gdfs\n",
    "    to_merge_vars = ['ComID', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode']\n",
    "    gdf_flow = gdf_fline.merge(gdf_fline_vaa[to_merge_vars], how='inner', on='ComID')\n",
    "    \n",
    "    gdf_flow_list.append(gdf_flow)\n",
    "\n",
    "gdf_flow = pd.concat(gdf_flow_list)\n",
    "\n",
    "# set crs\n",
    "gdf_flow = gdf_flow.set_crs(crs, inplace=True, allow_override=True)\n",
    "\n",
    "# subset to the target huc4\n",
    "gdf_flow_huc4 = gdf_flow.sjoin(gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4], how='inner', predicate='intersects')\n",
    "\n",
    "########## End Prepare flow lines ##########\n",
    "\n",
    "\n",
    "########## Read .nc files ##########\n",
    "conus_grid_nc = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "conus_reservoir_nc = f'{data_dir}/processed/LRR/input/reservoirs.nc'\n",
    "# Read CONUS grids\n",
    "with nc.Dataset(conus_grid_nc) as conus_grid:\n",
    "    lon_array = conus_grid.variables['lon'][:]\n",
    "    lat_array = conus_grid.variables['lat'][:]\n",
    "    grid_id_array = conus_grid.variables['id'][:, :]\n",
    "    flow_dir_array = conus_grid.variables['flow_dir'][:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhd_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def get_grids_in_hu(lon_array, lat_array, gdf_huc):\n",
    "    \"\"\"\n",
    "    Get grids (lon-lat) within the target HU\n",
    "    \n",
    "    gdf_huc: geodataframe of the target HU\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create lon-lat pairs from global vars: lon_array and lat_array\n",
    "    lon_lat_all = [[Point(lon, lat), i, j] for i, lon in enumerate(lon_array) for j, lat in enumerate(lat_array)]    # [Point(lon, lat), i-lon index, j-lat index]\n",
    "    \n",
    "    # index the wanted hu from the gdf\n",
    "    huc_geo = gdf_huc['geometry'].values   # the polygon for this huc\n",
    "        \n",
    "    # find (lon, lat) pairs within the area\n",
    "    lon_lat_sub = [i for i in lon_lat_all if huc_geo.contains(i[0])[0]]\n",
    "\n",
    "    # create point geodataframe for selected points and check\n",
    "    d = {'lon index': [i[1] for i in lon_lat_sub], 'lat index': [i[2] for i in lon_lat_sub]}\n",
    "    gdf_points = gpd.GeoDataFrame(d, \n",
    "                                  geometry=[i[0] for i in lon_lat_sub], crs='EPSG:4326')   # lon index, lat index, geometry\n",
    "    \n",
    "    result = {\n",
    "        'grids_in_hu': gdf_points,    # gdf - lon index, lat index, geometry; the index represents index in .nc files\n",
    "        'others': (lon_array, lat_array, gdf_huc)    # this is mainly for plot check\n",
    "    }\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def get_downstream_cell(i, j, direction):\n",
    "    \"\"\"\n",
    "    Returns the downstream cell coordinates based on D8 flow direction.\n",
    "    d8 directions:\n",
    "    32  64 128\n",
    "    16  x   1\n",
    "    8   4   2\n",
    "\n",
    "    Parameters:\n",
    "    i (int): Row index of the current cell\n",
    "    j (int): Column index of the current cell\n",
    "    direction (int): D8 flow direction code of the current cell\n",
    "\n",
    "    Returns:\n",
    "    tuple: Coordinates (row, col) of the downstream cell, or None if no downstream\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the changes in row (di) and column (dj) for each flow direction\n",
    "    # PLEASE note: the direction is defined as such, because the latitude array increases as the row index increases & longitude array increases as the col index increases!!!\n",
    "\n",
    "    direction_map = {\n",
    "        1: (0, 1),     # East: i, j+1\n",
    "        2: (-1, 1),    # Southeast: i-1, j+1\n",
    "        4: (-1, 0),    # South: i-1, j (move south, decrease row index)\n",
    "        8: (-1, -1),   # Southwest: i-1, j-1\n",
    "        16: (0, -1),   # West: i, j-1\n",
    "        32: (1, -1),   # Northwest: i+1, j-1\n",
    "        64: (1, 0),    # North: i+1, j (move north, increase row index)\n",
    "        128: (1, 1)    # Northeast: i+1, j+1\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get the changes in row and column for the given direction\n",
    "    di, dj = direction_map.get(direction, (0, 0))    # if direction is not in the direction_map, return (0, 0)\n",
    "\n",
    "    # If di and dj are both 0, it means the direction is not defined (e.g., a sink)\n",
    "    if di == 0 and dj == 0:\n",
    "        return None\n",
    "\n",
    "    # Calculate the coordinates of the downstream cell\n",
    "    downstream_i = i + di\n",
    "    downstream_j = j + dj\n",
    "    \n",
    "    return (downstream_i, downstream_j)\n",
    "\n",
    "# test\n",
    "get_downstream_cell(2, 16, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the grids within the target HU\n",
    "gdf_huc4 = gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4]\n",
    "gdf_grid_index_in_hu = get_grids_in_hu(lon_array, lat_array, gdf_huc4)['grids_in_hu']\n",
    "lon_index = gdf_grid_index_in_hu['lon index'].values    # contains duplicate values\n",
    "lat_index = gdf_grid_index_in_hu['lat index'].values    # contains duplicate values\n",
    "\n",
    "# Subset the flow direction array to the target HU basin (the rectangular area covering the HUC4 basin)\n",
    "# technically, I didn't \"subset\", just set the flow direction values outside the HU to -9999\n",
    "# first, set all conus grids outside the target HU to -9999\n",
    "mask = np.ones_like(flow_dir_array, dtype=bool)\n",
    "mask[lat_index, lon_index] = False\n",
    "flow_dir_array[mask] = -1    # set all conus grids outside the target HU to -1\n",
    "# second, subset the flow direction array to the target HU basin\n",
    "lat_index_unique = np.unique(lat_index)\n",
    "lon_index_unique = np.unique(lon_index)\n",
    "flow_dir_array_huc4 = flow_dir_array[np.ix_(lat_index_unique, lon_index_unique)]\n",
    "\n",
    "# also, get the grid id array for the target HU basin for record\n",
    "grid_id_array_huc4 = grid_id_array[np.ix_(lat_index_unique, lon_index_unique)]\n",
    "lat_array_huc4 = lat_array[lat_index_unique]\n",
    "lon_array_huc4 = lon_array[lon_index_unique]\n",
    "\n",
    "########## Sort the grids ##########\n",
    "\n",
    "# create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# add nodes\n",
    "nrows, ncols = flow_dir_array_huc4.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # add node for each grid\n",
    "        # if the node flows out of the huc4 basin, skip\n",
    "        if flow_dir_array_huc4[i, j] == -1:\n",
    "            continue\n",
    "        \n",
    "        G.add_node((i, j), \n",
    "                   flow_dir=flow_dir_array_huc4[i, j], grid_id=grid_id_array_huc4[i, j], grid_lat=lat_array_huc4[i], grid_lon=lon_array_huc4[j])\n",
    "\n",
    "        # determine downstream grids and add edges\n",
    "        downstream_grid_ij = get_downstream_cell(i, j, flow_dir_array_huc4[i, j])\n",
    "\n",
    "        if downstream_grid_ij is not None and flow_dir_array_huc4[downstream_grid_ij] != -1:\n",
    "            # if downstream grid is not None AND the downstream grid is not outside the huc4 basin\n",
    "            G.add_edge((i, j), downstream_grid_ij)\n",
    "\n",
    "# typological sorting\n",
    "sorted_grid_list = list(nx.topological_sort(G))\n",
    "\n",
    "# store the sorted grid id and upstream grid id for each grid: {grid: [upstream grid list]}\n",
    "# the grid order is following the topological sorting\n",
    "upstream_grid_dict = {grid: [] for grid in sorted_grid_list}\n",
    "for grid in sorted_grid_list:\n",
    "    upstream_grid_dict[grid] = list(G.predecessors(grid))\n",
    "# convert the grid index in upstream_grid_dict to grid id (the grid attribute in the node)\n",
    "upstream_grid_id_dict = {G.nodes[grid]['grid_id']: [G.nodes[upstream_grid]['grid_id'] for upstream_grid in upstream_grid_dict[grid]] for grid in upstream_grid_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "nodes = [(0,0), (0,1), (1,0), (2,0), (1,1)]\n",
    "G.add_nodes_from(nodes)\n",
    "edges = [((0,0), (1,0)), ((0,1), (1,0)), ((1,0), (2,0)), ((1,1), (2,0))]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# typological sorting\n",
    "sorted_grid_list = list(nx.topological_sort(G))\n",
    "\n",
    "# store the sorted grid id and upstream grid id for each grid: {grid: [upstream grid list]}\n",
    "# the grid order is following the topological sorting\n",
    "upstream_grid_dict = {grid: [] for grid in sorted_grid_list}\n",
    "for grid in sorted_grid_list:\n",
    "    upstream_grid_dict[grid] = list(G.predecessors(grid))\n",
    "\n",
    "upstream_grid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the graph on the map ---- #\n",
    "\n",
    "nodes_data = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "geometry = [Point(xy) for xy in zip(nodes_data['grid_lon'], nodes_data['grid_lat'])]\n",
    "gdf_nodes = gpd.GeoDataFrame(nodes_data, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "\n",
    "# ------------------- Plot ------------------- #\n",
    "# Variables to store components of arrows\n",
    "x = []\n",
    "y = []\n",
    "dx = []\n",
    "dy = []\n",
    "\n",
    "default_lon = lon_array_huc4[0]\n",
    "default_lat = lat_array_huc4[0]\n",
    "for edge in G.edges():\n",
    "    start_node = G.nodes[edge[0]]\n",
    "    end_node = G.nodes[edge[1]]\n",
    "    try:\n",
    "        x_start, y_start = start_node['grid_lon'], start_node['grid_lat']\n",
    "        x_end, y_end = end_node['grid_lon'], end_node['grid_lat']\n",
    "    except KeyError:\n",
    "        x_start, y_start = default_lon, default_lat\n",
    "        x_end, y_end = default_lon, default_lat\n",
    "\n",
    "    x.append(x_start)\n",
    "    y.append(y_start)\n",
    "    dx.append(x_end - x_start)\n",
    "    dy.append(y_end - y_start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot arrows\n",
    "ax.quiver(x, y, dx, dy, angles='xy', scale_units='xy', scale=1, color='gray')\n",
    "\n",
    "# Plot nodes\n",
    "gdf_nodes.plot(ax=ax, marker='o', color='black', alpha=0.7, markersize=10)\n",
    "\n",
    "gdf_huc4.plot(ax=ax, facecolor='none', edgecolor='tab:gray', linewidth=1)\n",
    "\n",
    "# plot flow line\n",
    "# specify to which stream order\n",
    "max_order = gdf_flow_huc4['StreamOrde'].max()\n",
    "min_order_to_keep = 4\n",
    "gdf_flow_huc4.loc[gdf_flow_huc4['StreamOrde']>=min_order_to_keep].plot(ax=ax, linewidth=1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.zeros((3, 5), dtype=[\n",
    "    ('var_1', int),\n",
    "    ('var_2', float),\n",
    "    ('var_3', str)\n",
    "])\n",
    "\n",
    "grid = np.zeros((3, 5), dtype=[('grid_id', int), ('elevation', float), ('slope', float), ('runoff', float), ('inflow', float), ('outflow', float), ('storage', float), ('has_reservoir', bool), ('reservoir_id', 'U10')])\n",
    "\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Daily NLDAS Runoff .nc to Single Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read NLDAS daily runoff data & combine to a single file ---- #\n",
    "\n",
    "nldas_runoff_dir = f'{data_dir}/processed/nldas_daily'\n",
    "\n",
    "nc_files_list = [f for f in os.listdir(nldas_runoff_dir) if f.endswith('.nc')]\n",
    "date_list = pd.to_datetime([f.split('.')[0] for f in nc_files_list]).sort_values().tolist()\n",
    "\n",
    "# get basic dimension info from the first nc file\n",
    "with nc.Dataset(f'{nldas_runoff_dir}/{nc_files_list[0]}') as daily_runoff:\n",
    "    lat_array_conus = daily_runoff.variables['lat'][:]\n",
    "    lon_array_conus = daily_runoff.variables['lon'][:]\n",
    "\n",
    "# create a new nc dataset\n",
    "combined_nc_file_path = f'{data_dir}/processed/LRR/input/nldas_runoff.nc'\n",
    "with nc.Dataset(combined_nc_file_path, 'w') as combined_nc:\n",
    "    # create dimensions\n",
    "    combined_nc.createDimension('time', len(date_list))\n",
    "    combined_nc.createDimension('lat', len(lat_array_conus))\n",
    "    combined_nc.createDimension('lon', len(lon_array_conus))\n",
    "\n",
    "    # create variables\n",
    "    combined_nc.createVariable('time', 'S10', ('time',))\n",
    "    combined_nc.createVariable('lat', 'f4', ('lat',))\n",
    "    combined_nc.createVariable('lon', 'f4', ('lon',))\n",
    "    combined_nc.createVariable('Qs', 'f8', ('time', 'lat', 'lon',))\n",
    "    combined_nc.createVariable('Qsb', 'f8', ('time', 'lat', 'lon',))\n",
    "\n",
    "    # variable attributes\n",
    "    combined_nc.variables['time'].units = 'none'\n",
    "    combined_nc.variables['time'].long_name = 'string datetime (yyyy-mm-dd) from 1980-01-01 to 2019-12-31'\n",
    "    combined_nc.variables['lat'].long_name = 'latitude'\n",
    "    combined_nc.variables['lon'].long_name = 'longitude'\n",
    "    combined_nc.variables['Qs'].long_name = 'surface runoff (mm/d)'\n",
    "    combined_nc.variables['Qsb'].long_name = 'baseflow (mm/d)'\n",
    "\n",
    "    # write data\n",
    "    combined_nc.variables['time'][:] = np.array(date_list).astype('datetime64[D]').astype(str)\n",
    "    combined_nc.variables['lat'][:] = lat_array_conus\n",
    "    combined_nc.variables['lon'][:] = lon_array_conus\n",
    "\n",
    "    # write runoff data from each nc file\n",
    "    for i, date in enumerate(date_list):\n",
    "        if i % 100 == 0:\n",
    "            print(f'Processing {date}')\n",
    "        year = date.year\n",
    "        month = f'{date.month:02d}'\n",
    "        day = f'{date.day:02d}'\n",
    "\n",
    "        nc_file_path = f'{nldas_runoff_dir}/{year}{month}{day}.nc'\n",
    "        with nc.Dataset(nc_file_path) as daily_runoff:\n",
    "            # PLEASE NOTE FOR THE RUNOFF DATA\n",
    "            # The runoff data in the original nc files are hourly average values for each day (mm/hr)\n",
    "            # So, here, I need to multiply the values by 24 to get the daily values (mm/day)\n",
    "            combined_nc.variables['Qs'][date_list.index(date), :, :] = daily_runoff.variables['Qs'][:] * 24    # convert from mm/hr to mm/day\n",
    "            combined_nc.variables['Qsb'][date_list.index(date), :, :] = daily_runoff.variables['Qsb'][:] * 24    # convert from mm/hr to mm/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Test the runoff data ---- #\n",
    "\n",
    "with nc.Dataset(combined_nc_file_path) as conus_runoff:\n",
    "    # Qs = conus_runoff.variables['Qs'][:]\n",
    "    # Qsb = conus_runoff.variables['Qsb'][:]\n",
    "\n",
    "    time = conus_runoff.variables['time'][:]\n",
    "\n",
    "start_date = '1980-01-01'\n",
    "end_date = '1980-12-31'\n",
    "start_date_index = np.where(time==start_date)[0][0]\n",
    "end_date_index = np.where(time==end_date)[0][0]\n",
    "\n",
    "time_sub = time[start_date_index:end_date_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Observed Storage for Assimilation\n",
    "Concat all storage series of the 452 reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ts_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/DROM_CONUS_Analysis/Data/HydroShare/data_training'\n",
    "res_meta_file = f'/Users/donghui/Box Sync/Research/PhD/Projects/DROM_CONUS_Analysis/Data/HydroShare/reservoir_metadata.csv'\n",
    "\n",
    "df_res_meta = pd.read_csv(res_meta_file) \n",
    "\n",
    "# Initialize a dataframe\n",
    "df_storage_all = pd.DataFrame()\n",
    "\n",
    "res_ts_files = [f for f in os.listdir(res_ts_dir) if f.endswith('.csv')]\n",
    "for res_ts_file in res_ts_files:\n",
    "    gid = res_ts_file.split('.')[0]\n",
    "    df_res_i = pd.read_csv(f'{res_ts_dir}/{res_ts_file}')\n",
    "    df_res_i['Time'] = pd.to_datetime(df_res_i['Time'])\n",
    "    df_res_i.set_index('Time', inplace=True)\n",
    "\n",
    "    # remove duplicate indices\n",
    "    df_res_i = df_res_i[~df_res_i.index.duplicated(keep='first')]\n",
    "\n",
    "    # use metadata to convert normalized storage to actual storage\n",
    "    max_storage = df_res_meta.loc[df_res_meta['ID']==int(gid), 'Maximum Storage'].values[0]\n",
    "    df_res_i['Storage'] = df_res_i['Storage'] * max_storage\n",
    "\n",
    "    # merge to the combined dataframe\n",
    "    # rename for merge\n",
    "    df_res_i.rename(columns={'Storage': gid}, inplace=True)\n",
    "    df_storage_all = df_storage_all.merge(df_res_i[gid], how='outer', left_index=True, right_index=True)\n",
    "\n",
    "    # replace all NaN with -9999\n",
    "    df_storage_all.fillna(-9999, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage_all.to_csv(f'{data_dir}/processed/LRR/input/reservoir_storage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_dir}/processed/LRR/input/reservoir_storage.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_storage_assimilation(storage_file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the reservoir storage time series file for assimilation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_storage_assimilation : pd.DataFrame\n",
    "        index: Time [pd.Timestamp]\n",
    "        cols: 'Reservoir gid' - reservoir storage [acft]\n",
    "    \"\"\"\n",
    "\n",
    "    df_storage_assimilation = pd.read_csv(storage_file_path, index_col=0)\n",
    "    df_storage_assimilation.index = pd.to_datetime(df_storage_assimilation.index)\n",
    "    return df_storage_assimilation\n",
    "\n",
    "storage_file_path = f'{data_dir}/processed/LRR/input/reservoir_storage.csv'\n",
    "df_storage_assimilation = read_storage_assimilation(storage_file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
