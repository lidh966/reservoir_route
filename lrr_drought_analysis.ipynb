{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as mtick\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, SpectralClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Water_Supply_Drought'\n",
    "data_dir = f'{base_dir}/data'\n",
    "reservoir_data_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/DROM_CONUS_Analysis/Data/HydroShare'\n",
    "output_dir = f'{data_dir}/results/lrr_output'\n",
    "\n",
    "os.chdir(base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The analysis is by each river basin (huc4)\n",
    "# Specify basin huc4 here for following analysis\n",
    "huc4 = '1203'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Prepare geospatial files ---- #\n",
    "\n",
    "nhd_data_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Drought_Cycle_Analysis/Data'\n",
    "crs = 'EPSG:4326'\n",
    "huc2_conus = [f'0{i}' if i<10 else f'{i}' for i in range(1, 19)]\n",
    "\n",
    "# read reservoirs\n",
    "df = pd.read_excel(f'{data_dir}/processed/reservoirs.xlsx', dtype={'huc4':str})\n",
    "gdf_reservoirs = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "gdf_reservoirs.set_crs(crs, inplace=True, allow_override=True)\n",
    "\n",
    "# read HUCs\n",
    "huc2 = huc4[0:2]\n",
    "gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "gdf_huc2_all = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "gdf_huc4_all = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "gdf_huc6_all = gpd.read_file(gdb_file, layer='WBDHU6')\n",
    "gdf_huc8_all = gpd.read_file(gdb_file, layer='WBDHU8')\n",
    "gdf_huc10_all = gpd.read_file(gdb_file, layer='WBDHU10')\n",
    "\n",
    "# set crs\n",
    "gdf_huc2_all = gdf_huc2_all.set_crs(crs, inplace=False, allow_override=True)    # includes the huc2 region\n",
    "gdf_huc4_all = gdf_huc4_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc4 subregions in this huc2 region\n",
    "gdf_huc6_all = gdf_huc6_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc6 basins in this huc2 region\n",
    "gdf_huc8_all = gdf_huc8_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc8 subbasins in this huc2 region\n",
    "gdf_huc10_all = gdf_huc10_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc10 subbasins in this huc2 region\n",
    "\n",
    "########## Prepare flow lines ##########\n",
    "\n",
    "if huc2 == '03':    # multiple NHDP files for 03\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['N','S','W']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['N','S','W']]\n",
    "elif huc2 == '10': \n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['U','L']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['U','L']]\n",
    "else:\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDPlusAttributes']\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDSnapshot/Hydrography']\n",
    "\n",
    "gdf_flow_list = []\n",
    "for flow_attr_file, hydro_file in zip(flow_attr_file_list, hydro_file_list):\n",
    "    gdf_fline_vaa = gpd.read_file(flow_attr_file, layer='PlusFlowlineVAA')\n",
    "    gdf_fline = gpd.read_file(hydro_file, layer='NHDFlowline')\n",
    "\n",
    "    # change COMID to ComID if the error exists\n",
    "    if not 'ComID' in gdf_fline:\n",
    "        gdf_fline.rename(columns={'COMID':'ComID'}, inplace=True)\n",
    "\n",
    "    # change vaa file ComID to int\n",
    "    to_int_var = ['ComID', 'StreamOrde', 'StreamCalc']\n",
    "    gdf_fline_vaa[to_int_var] = gdf_fline_vaa[to_int_var].astype(int)\n",
    "\n",
    "    # merge this two gdfs\n",
    "    to_merge_vars = ['ComID', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode']\n",
    "    gdf_flow = gdf_fline.merge(gdf_fline_vaa[to_merge_vars], how='inner', on='ComID')\n",
    "    \n",
    "    gdf_flow_list.append(gdf_flow)\n",
    "\n",
    "gdf_flow = pd.concat(gdf_flow_list)\n",
    "\n",
    "# set crs\n",
    "gdf_flow = gdf_flow.set_crs(crs, inplace=True, allow_override=True)\n",
    "\n",
    "# subset to the target huc4\n",
    "gdf_flow_huc4 = gdf_flow.sjoin(gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4], how='inner', predicate='intersects')\n",
    "\n",
    "########## End Prepare flow lines ##########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Supply Drought Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Water Supply Drought Risk & Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot water deficit for group meeting\n",
    "\"\"\"\n",
    "\n",
    "ds = xr.open_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "\n",
    "# get the grid indices that locate in the huc4: flow_direction != -1\n",
    "huc4_lat_lon_tup = np.where(ds['flow_direction'] != -1)    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "# change water deficit to nan if flow direction is -1\n",
    "ds['water_deficit'] = ds['water_deficit'].where(ds['flow_direction'] != -1)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# plot huc4 basin as background\n",
    "gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4].plot(ax=ax, facecolor='whitesmoke', edgecolor='none')\n",
    "\n",
    "# plot sustainability index\n",
    "ds['water_deficit'].sel(time='2000-09-01').plot(ax=ax, cmap='Reds', alpha=1)\n",
    "\n",
    "# plot huc4 basin\n",
    "gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4].plot(ax=ax, facecolor='none', edgecolor='gray')\n",
    "\n",
    "# plot flow lines\n",
    "max_order = gdf_flow_huc4['StreamOrde'].max()\n",
    "min_order_to_keep = 4\n",
    "gdf_flow_huc4.loc[gdf_flow_huc4['StreamOrde']>=min_order_to_keep].plot(ax=ax, linewidth=1, color='tab:blue')\n",
    "\n",
    "# plot reservoirs\n",
    "gdf_reservoirs.loc[gdf_reservoirs['huc4']==huc4].plot(ax=ax, color='tab:gray', marker='v', markersize=80)\n",
    "\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/water_deficit_{huc4}.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_flow_huc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['water_deficit'].isel(time=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define functions\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "drought_index = deficit_volume[event_E] / baseline_volume\n",
    "baseline_volume = mean of all deficit volumes in the entire simulation period\n",
    "drought_risk = sum: drought_index * prob(drought event)\n",
    "prob(drought event) = count(drought event) / total count\n",
    "\"\"\"\n",
    "\n",
    "def cal_baseline_volume(deficit_array_entire_period, threshold=0.00001):\n",
    "    \"\"\"\n",
    "    Calculate the baseline deficit volume.\n",
    "        here, I use the average deficit volume of each drought event in the entire simulation period\n",
    "        a drought event is defined as a period of consecutive days with deficit volume > 0\n",
    "\n",
    "    Args:\n",
    "        deficit_array_entire_period: numpy array\n",
    "            daily deficit volume of each grid cell in the entire simulation period\n",
    "\n",
    "    Returns:\n",
    "        baseline_volume: float\n",
    "            the average deficit volume of each drought event in the entire simulation period\n",
    "    \"\"\"\n",
    "\n",
    "    # find drought events, by splitting the deficit_array_entire_period by the negative values\n",
    "    drought_events = np.split(deficit_array_entire_period, np.where(deficit_array_entire_period < threshold)[0])    # list of subarrays\n",
    "    drought_events = [event for event in drought_events if len(event) > 1]\n",
    "    drought_events = [event[1:] for event in drought_events]    # remove the first element of each subarray, which is the negative value\n",
    "\n",
    "    # if there is only one drought event, it means that the entire period is a drought event\n",
    "    # TODO: refine this\n",
    "    if len(drought_events) == 1:\n",
    "        print('The entire period is a drought event.')\n",
    "        return np.sum(drought_events[0])\n",
    "    \n",
    "    if deficit_array_entire_period[-1] > 0:\n",
    "        drought_events = drought_events[:-1]    # remove the last subarray, which can be an incomplete drought event\n",
    "\n",
    "\n",
    "    # calculate the average deficit volume of each drought event in the entire simulation period\n",
    "    baseline_volume = np.mean([np.sum(event) for event in drought_events])\n",
    "\n",
    "    return baseline_volume\n",
    "\n",
    "def cal_drought_risk(deficit_array, baseline_deficit, threshold=0.00001):\n",
    "    \"\"\"\n",
    "    Calculate drought risk over a period of time, e.g., every 15-year moving window.\n",
    "\n",
    "    Args:\n",
    "        deficit_array: numpy array\n",
    "            daily deficit volume of each grid cell in the given period of time\n",
    "        baseline_deficit: float\n",
    "\n",
    "    Returns:\n",
    "        drought_risk: float\n",
    "    \"\"\"\n",
    "\n",
    "    if np.all(deficit_array < threshold):\n",
    "        return 0\n",
    "    \n",
    "    # find drought events, by splitting the deficit_array by the negative values\n",
    "    # TODO: wrap this into a function\n",
    "    drought_events = np.split(deficit_array, np.where(deficit_array < threshold)[0])    # list of subarrays\n",
    "    drought_events = [event for event in drought_events if len(event) > 1]\n",
    "    drought_events = [event[1:] for event in drought_events]    # remove the first element of each subarray, which is the negative value\n",
    "\n",
    "    # if there is only one drought event, it means that the entire period is a drought event\n",
    "    # TODO: refine this\n",
    "    if len(drought_events) == 1:\n",
    "        return np.sum(drought_events[0]) / baseline_deficit\n",
    "    \n",
    "    if deficit_array[-1] > 0:\n",
    "        drought_events = drought_events[:-1]    # remove the last subarray, which can be an incomplete drought event\n",
    "\n",
    "    if len(drought_events) == 0:\n",
    "        return 0\n",
    "\n",
    "    # calculate drought risk\n",
    "    drought_index = [np.sum(event) / baseline_deficit for event in drought_events]\n",
    "    prob_drought_event = 1 / len(drought_events)    # TODO: replace with the actual probability of each event\n",
    "    drought_risk = np.sum([drought_index[i] * prob_drought_event for i in range(len(drought_index))])\n",
    "\n",
    "    return drought_risk\n",
    "\n",
    "    \n",
    "# test\n",
    "deficit_array_entire_period = np.array([1, -1, -2, 1, 2, 3, -1, -2, -3, -4, 1, 2, 3, -1, -2, -3, 7, 7, 4, -1])\n",
    "baseline_deficit = cal_baseline_volume(deficit_array_entire_period)\n",
    "print(baseline_deficit)\n",
    "\n",
    "# # test\n",
    "# deficit_array = np.array([1, -1, -2, 1, 2, 3, -1, -2, -3, -4, 1, 2, 3, -1, -2, -3, 7, 7, 4, -1, 1, 2, 100, -5])\n",
    "# drought_risk = cal_drought_risk(deficit_array, baseline_deficit)\n",
    "# print(drought_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate water deficit drought risk for each grid cell in a moving window\n",
    "    On a monthly basis\n",
    "\"\"\"\n",
    "\n",
    "# Convert warnings to exceptions\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "huc4_list = ['0108', '0110', '0301', '0305', '0306', '0313', '0315', '0505', '0507', '0509', '0511', '0512', '0513', '0601', '0602', '0603', '0701', '0702', '0708', '0710', '0714', '0902', '0804', '1003', '1008', '1012', '1018', '1025', '1026', '1027', '1028', '1029', '1101', '1102', '1107', '1111', '1114', '1201', '1202', '1203', '1210', '1302', '1307', '1401', '1402', '1404', '1506', '1602', '1703', '1704', '1705', '1709', '1711', '1802', '1803', '1804']\n",
    "\n",
    "for huc4 in huc4_list:\n",
    "    print(f'---- Processing {huc4} ----')\n",
    "    # read simulation results\n",
    "    try:\n",
    "        ds = xr.load_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "    except FileNotFoundError:\n",
    "        print(f'No simulation results for {huc4}')\n",
    "        continue\n",
    "\n",
    "    # Omit the first 2 years of spinup\n",
    "    spinup_start = '1988-01-01'\n",
    "    spinup_end = '1990-01-01'\n",
    "    spinup_len = (pd.to_datetime(spinup_end) - pd.to_datetime(spinup_start)).days\n",
    "    # skip the first 2 years of spinup\n",
    "    ds = ds.isel(time=slice(spinup_len, None))\n",
    "\n",
    "    # get the grid indices that locate in the huc4: flow_direction != -1\n",
    "    huc4_lat_lon_tup = np.where(ds['flow_direction'] != -1)    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "    # loop through each grid cell & calculate moving-window reliability, resilience, vulnerability for each grid cell\n",
    "    window_size_year = 10    # 10 years\n",
    "    window_size = 12 * window_size_year    # 10 years -> 3650 days\n",
    "    drought_risk_dict = {f'{i}_{j}': None for i, j in zip(huc4_lat_lon_tup[0], huc4_lat_lon_tup[1])}    # key: lat_lon, value: drought risk series\n",
    "\n",
    "    for i, (lat_ind, lon_ind) in enumerate(zip(huc4_lat_lon_tup[0], huc4_lat_lon_tup[1])):\n",
    "        if i % 100 == 0:\n",
    "            print(f'Processing {i}th grid cell...')\n",
    "\n",
    "        # get water deficit time series over the entire simulation period\n",
    "        deficit_array_entire_period = ds['water_deficit'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "        # if all values are below 0.000001, then skip this grid cell - no drought risk\n",
    "        if np.all(deficit_array_entire_period < 0.000001):\n",
    "            continue\n",
    "        \n",
    "        # ELSE, calculate the moving-window drought risk\n",
    "        # convert to pandas series\n",
    "        deficit_series = pd.Series(deficit_array_entire_period, index=pd.to_datetime(ds['time'].values))\n",
    "        \n",
    "        # convert to monthly\n",
    "        deficit_series = deficit_series.resample('M').sum()\n",
    "\n",
    "        # get the baseline deficit volume\n",
    "        try: \n",
    "            baseline_deficit = cal_baseline_volume(deficit_series.values)\n",
    "        except RuntimeWarning:\n",
    "            print(f'RuntimeWarning: {i}: {lat_ind}_{lon_ind}')\n",
    "            import sys\n",
    "            sys.exit()\n",
    "\n",
    "        # Calculate moving-window drought risk\n",
    "        drought_risk_series = deficit_series.rolling(window_size, center=False).apply(cal_drought_risk, args=(baseline_deficit,), raw=True)\n",
    "\n",
    "        # append to dictionary\n",
    "        drought_risk_dict[f'{lat_ind}_{lon_ind}'] = drought_risk_series\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_risk_huc4 = pd.DataFrame(drought_risk_dict)\n",
    "    df_risk_huc4.index.name = 'time'\n",
    "\n",
    "    # save to file\n",
    "    df_risk_huc4.to_csv(f'data/results/basin_drought_risk/{huc4}.csv')\n",
    "\n",
    "    # clear memory\n",
    "    del ds, df_risk_huc4\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Drought Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('data/results/basin_drought_risk') if f.endswith('.csv')]\n",
    "files.sort()\n",
    "\n",
    "# initialialize a dataframe to store the monthly average drought risk for each basin\n",
    "df_risk = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    huc4 = file.split('.')[0]\n",
    "    df = pd.read_csv(f'data/results/basin_drought_risk/{file}', index_col='time')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # get monthly average drought risk for each month\n",
    "    df['mean'] = df.mean(axis=1)\n",
    "\n",
    "    # add prefix to all column names\n",
    "    df = df.add_prefix(f'{huc4}_')\n",
    "\n",
    "    df_risk = df_risk.merge(df[f'{huc4}_mean'], how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot df_risk columns that starts with 'huc2'\n",
    "huc2_list = list(set([file.split('.')[0][0:2] for file in files]))\n",
    "for huc2 in huc2_list:\n",
    "    print(f'---- Plotting {huc2} ----')\n",
    "\n",
    "    df_risk_huc2 = df_risk[df_risk.columns[df_risk.columns.str.startswith(huc2)]]\n",
    "\n",
    "    # plot\n",
    "    plt.style.use('default')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    df_risk_huc2.plot(ax=ax, legend=None, color='lightgrey', alpha=0.5, label=None)\n",
    "    df_risk_huc2.mean(axis=1).plot(ax=ax, color='black', linewidth=2, label='Mean')\n",
    "\n",
    "    ax.set_xlabel('Time', fontweight='bold')\n",
    "    ax.set_xlim(pd.to_datetime('2000-01-01'), pd.to_datetime('2019-12-31'))\n",
    "\n",
    "    ax.set_ylabel('Drought Risk', fontweight='bold')\n",
    "    ax.set_ylim(0, 2.1)\n",
    "\n",
    "    ax.legend()\n",
    "    # only keep the mean line in the legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=[handles[0], handles[-1]], labels=['Inividual basin', 'Basin average'])\n",
    "\n",
    "    plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/drought_risk_{huc2}.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustainability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define reliability, resilience, vulnerability ---- #\n",
    "\n",
    "def cal_reliability(deficit_array, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Calculate reliability of water deficit time series for a given grid.\n",
    "    \n",
    "    threshold: the threshold of water deficit, above which to be considered as a deficit\n",
    "    \"\"\"\n",
    "\n",
    "    n_deficit = deficit_array[deficit_array > threshold].size\n",
    "\n",
    "    reliability = 1 - n_deficit / deficit_array.size\n",
    "\n",
    "    return(reliability)\n",
    "\n",
    "def cal_resilience(deficit_array, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Calculate resilience of water deficit time series for a given grid.\n",
    "        Resilience: number of times a non-deficit day follows a deficit day / number of deficit days\n",
    "    \n",
    "    threshold: the threshold of water deficit, above which to be considered as a deficit\n",
    "    \"\"\"\n",
    "\n",
    "    n_deficit = deficit_array[deficit_array > threshold].size\n",
    "    if n_deficit == 0:    # if there is no deficit, no resilience\n",
    "        return np.nan\n",
    "    \n",
    "    n_resilience = 0\n",
    "\n",
    "    for i in range(deficit_array.size - 1):\n",
    "        if deficit_array[i] > threshold and deficit_array[i + 1] <= threshold:\n",
    "            n_resilience += 1\n",
    "\n",
    "    resilience = n_resilience / n_deficit\n",
    "    \n",
    "    return(resilience)\n",
    "\n",
    "def cal_vulnerability(deficit_array, threshold=0.0001):\n",
    "    \"\"\"\n",
    "    Calculate vulnerability of water deficit time series for a given grid.\n",
    "        Vulnerability: sum of deficit amount / number of deficit days\n",
    "    \n",
    "    threshold: the threshold of water deficit, above which to be considered as a deficit\n",
    "    \"\"\"\n",
    "\n",
    "    n_deficit = deficit_array[deficit_array > threshold].size\n",
    "    if n_deficit == 0:    # if there is no deficit, no vulnerability\n",
    "        return np.nan\n",
    "\n",
    "    vulnuerability = np.sum(deficit_array[deficit_array > threshold]) / n_deficit\n",
    "\n",
    "    return(vulnuerability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving-window sustainability index & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read simulation results\n",
    "ds = xr.load_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "\n",
    "# Omit the first 2 years of spinup\n",
    "spinup_start = '1988-01-01'\n",
    "spinup_end = '1990-01-01'\n",
    "spinup_len = (pd.to_datetime(spinup_end) - pd.to_datetime(spinup_start)).days\n",
    "# skip the first 2 years of spinup\n",
    "ds = ds.isel(time=slice(spinup_len, None))\n",
    "\n",
    "# get the grid indices that locate in the huc4: flow_direction != -1\n",
    "huc4_lat_lon_tup = np.where(ds['flow_direction'] != -1)    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "# loop through each grid cell & calculate moving-window reliability, resilience, vulnerability for each grid cell\n",
    "window_size_year = 15    # 10 years\n",
    "window_size = 365 * window_size_year    # 10 years -> 3650 days\n",
    "reliability_list = []    # list of reliability Series (moving-window) for each grid cell\n",
    "resilience_list = []    # list of resilience Series (moving-window) for each grid cell\n",
    "vulnerability_list = []    # list of vulnerability Series (moving-window) for each grid cell\n",
    "\n",
    "for lat_ind, lon_ind in zip(huc4_lat_lon_tup[0], huc4_lat_lon_tup[1]):\n",
    "    # get water deficit time series\n",
    "    deficit_array = ds['water_deficit'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "    # convert to pandas series\n",
    "    deficit_series = pd.Series(deficit_array, index=ds['time'].values)\n",
    "    # convert index to datetime\n",
    "    deficit_series.index = pd.to_datetime(deficit_series.index)\n",
    "    \n",
    "    # Calculate moving-window reliability, resilience, vulnerability\n",
    "    reliability_rolling_series = deficit_series.rolling(window_size, center=True).apply(cal_reliability, raw=True)\n",
    "    resilience_rolling_series = deficit_series.rolling(window_size, center=True).apply(cal_resilience, raw=True)\n",
    "\n",
    "    # append to list\n",
    "    reliability_list.append(reliability_rolling_series)\n",
    "    resilience_list.append(resilience_rolling_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Temply save the results to csv ---- #\n",
    "\n",
    "df_reliability = pd.concat(reliability_list, axis=1)\n",
    "df_resilience = pd.concat(resilience_list, axis=1)\n",
    "\n",
    "# save to csv\n",
    "df_reliability.to_csv(f'{output_dir}/{huc4}/reliability_{huc4}_{window_size_year}.csv')\n",
    "df_resilience.to_csv(f'{output_dir}/{huc4}/resilience_{huc4}_{window_size_year}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot reliability, resilience, vulnerability time series for all basins in one plot ---- #\n",
    "\n",
    "huc4_list = ['1709', '1705', '1804', '1203']\n",
    "fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(25, 10), sharex=False, sharey=False)\n",
    "\n",
    "for i, huc4 in enumerate(huc4_list):\n",
    "\n",
    "    # Plot for reliability\n",
    "    df_reliability = pd.read_csv(f'{output_dir}/{huc4}/reliability_{huc4}.csv', index_col=0, parse_dates=True)\n",
    "    df_resilience = pd.read_csv(f'{output_dir}/{huc4}/resilience_{huc4}.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "    df_reliability_month = df_reliability.resample('MS').mean()\n",
    "    # df_reliability_month.plot(ax=ax[0, i], color='whitesmoke', legend=False)\n",
    "    # plot mean & median\n",
    "    df_reliability_month.mean(axis=1).plot(ax=ax[0, i], color='tab:blue', linewidth=2, label='Average')\n",
    "    df_reliability_month.median(axis=1).plot(ax=ax[0, i], color='tab:red', linewidth=2, label='Median')\n",
    "    # plot 25% & 75% quantile\n",
    "    df_reliability_month.quantile(0.25, axis=1).plot(ax=ax[0, i], color='tab:orange', linewidth=2, linestyle='--', label='Lower Quartile')\n",
    "    df_reliability_month.quantile(0.75, axis=1).plot(ax=ax[0, i], color='tab:orange', linewidth=2, linestyle='--', label='Upper Quartile')\n",
    "\n",
    "    # Plot for resilience\n",
    "    df_resilience_month = df_resilience.resample('MS').mean()\n",
    "    # df_resilience_month.plot(ax=ax[1, i], color='whitesmoke', legend=False)\n",
    "    # plot mean & median\n",
    "    df_resilience_month.mean(axis=1).plot(ax=ax[1, i], color='tab:blue', linewidth=2, label='Average')\n",
    "    df_resilience_month.median(axis=1).plot(ax=ax[1, i], color='tab:red', linewidth=2, label='Median')\n",
    "    # plot 25% & 75% quantile\n",
    "    df_resilience_month.quantile(0.25, axis=1).plot(ax=ax[1, i], color='tab:orange', linewidth=2, linestyle='--', label='25th Percentile')\n",
    "    df_resilience_month.quantile(0.75, axis=1).plot(ax=ax[1, i], color='tab:orange', linewidth=2, linestyle='--', label='75th Percentile')\n",
    "\n",
    "    # # Plot drought events\n",
    "    # df_pdsi = pd.read_csv(f'{data_dir}/processed/LRR/input/pdsi_{huc4}.csv', index_col=0, parse_dates=True)\n",
    "    # # find drought periods: PDSI < -1\n",
    "    # drought_periods = []\n",
    "    # for i in range(df_pdsi.size - 1):\n",
    "    #     if df_pdsi.iloc[i, 0] < 0 and df_pdsi.iloc[i+1, 0] < -1:\n",
    "    #         drought_periods.append(df_pdsi.index[i])\n",
    "\n",
    "    # # plot drought periods\n",
    "    # for drought_period in drought_periods:\n",
    "    #     if drought_period < df_reliability_month.dropna().index[0] or drought_period > df_reliability_month.dropna().index[-1]:\n",
    "    #         continue\n",
    "    #     ax[0, i].axvspan(drought_period, drought_period + pd.DateOffset(months=1), color='lightgray', alpha=0.5)\n",
    "    #     ax[1, i].axvspan(drought_period, drought_period + pd.DateOffset(months=1), color='lightgray', alpha=0.5)\n",
    "\n",
    "    ax[0, i].set_xlim([df_reliability_month.dropna().index[0], df_reliability_month.dropna().index[-1]])\n",
    "    ax[1, i].set_xlim([df_reliability_month.dropna().index[0], df_reliability_month.dropna().index[-1]])\n",
    "    \n",
    "\n",
    "ax[0,0].legend(loc='lower left', fontsize=14)\n",
    "\n",
    "# ax[1,0].set_ylim([0, 0.4])\n",
    "# ax[1,1].set_xlim([df_reliability_month.dropna().index[0], df_reliability_month.dropna().index[-1]])\n",
    "\n",
    "ax[0,0].set_ylabel('Reliability', fontsize=14, fontweight='bold')\n",
    "ax[1,0].set_ylabel('Resilience', fontsize=14, fontweight='bold')\n",
    "\n",
    "fig.supxlabel('Date', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'{output_dir}/reliability_resilience_nosharexy.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find drought periods for each basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Find drought periods using PDSI ---- #\n",
    "\n",
    "huc4 = '1203'\n",
    "# read PDSI\n",
    "df_pdsi = pd.read_csv(f'{data_dir}/processed/LRR/input/pdsi_{huc4}.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# find drought periods: PDSI < 0\n",
    "drought_periods = []\n",
    "for i in range(df_pdsi.size - 1):\n",
    "    if df_pdsi.iloc[i, 0] < 0 and df_pdsi.iloc[i+1, 0] < -1:\n",
    "        drought_periods.append(df_pdsi.index[i])\n",
    "\n",
    "# ---- Plot PDSI & drought periods ---- #\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "df_pdsi.plot(ax=ax, color='whitesmoke', legend=False)\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=2)\n",
    "ax.axhline(y=-4, color='black', linestyle='--', linewidth=2)\n",
    "ax.axhline(y=-8, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "for drought_period in drought_periods:\n",
    "    ax.axvspan(drought_period, drought_period + pd.DateOffset(months=1), color='tab:red', alpha=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drought_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reliability_pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot reliability, resilience, vulnerability time series ---- #\n",
    "\n",
    "date_range = reliability_list[0].index    # date range of the moving-window sustainability metrics\n",
    "reliability_array = np.array(reliability_list)    # (n_grid, n_window)\n",
    "# plot reliability time series with confidence interval\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# plot reliability time series of median and mean\n",
    "ax.plot(date_range, np.nanmedian(reliability_array, axis=0), label='Median', color='tab:red')\n",
    "ax.plot(date_range, np.nanmean(reliability_array, axis=0), label='Mean', color='tab:orange')\n",
    "# plot reliability time series of 25th and 75th percentile\n",
    "ax.plot(date_range, np.nanpercentile(reliability_array, 25, axis=0), linestyle='--', label='25th percentile', color='tab:blue')\n",
    "ax.plot(date_range, np.nanpercentile(reliability_array, 75, axis=0), linestyle='--', label='75th percentile', color='tab:blue')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Reliability')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "resilience_array = np.array(resilience_list)    # (n_grid, n_window)\n",
    "# plot resilience time series with confidence interval\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# plot resilience time series of median and mean\n",
    "ax.plot(date_range, np.nanmedian(resilience_array, axis=0), label='Median', color='tab:red')\n",
    "ax.plot(date_range, np.nanmean(resilience_array, axis=0), label='Mean', color='tab:orange')\n",
    "# plot resilience time series of 25th and 75th percentile\n",
    "ax.plot(date_range, np.nanpercentile(resilience_array, 25, axis=0), linestyle='--', label='25th percentile', color='tab:blue')\n",
    "ax.plot(date_range, np.nanpercentile(resilience_array, 75, axis=0), linestyle='--', label='75th percentile', color='tab:blue')\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Resilience')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sns.load_dataset(\"flights\")\n",
    "flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate sustainability index (entire period) & add to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Calculate sustainability index & add to dataset ---- #\n",
    "\n",
    "# read simulation results\n",
    "ds = xr.load_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "\n",
    "# Omit the first 2 years of spinup\n",
    "spinup_start = '1988-01-01'\n",
    "spinup_end = '1990-01-01'\n",
    "spinup_len = (pd.to_datetime(spinup_end) - pd.to_datetime(spinup_start)).days\n",
    "# skip the first 2 years of spinup\n",
    "ds = ds.isel(time=slice(spinup_len, None))\n",
    "\n",
    "# get the grid indices that locate in the huc4: flow_direction != -1\n",
    "huc4_lat_lon_tup = np.where(ds['flow_direction'] != -1)    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "# loop through each grid cell & calculate sustainability index & add to the ds\n",
    "reliability_array = np.ones_like(ds['water_deficit'].isel(time=0).values) * np.nan\n",
    "resilience_array = np.ones_like(ds['water_deficit'].isel(time=0).values) * np.nan\n",
    "vulnerability_array = np.ones_like(ds['water_deficit'].isel(time=0).values) * np.nan\n",
    "for lat_ind, lon_ind in zip(huc4_lat_lon_tup[0], huc4_lat_lon_tup[1]):\n",
    "    # get the water deficit time series\n",
    "    deficit_array = ds['water_deficit'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "    \n",
    "    # calculate sustainability index\n",
    "    reliability = cal_reliability(deficit_array)\n",
    "    resilience = cal_resilience(deficit_array)\n",
    "    vulnerability = cal_vulnerability(deficit_array)\n",
    "\n",
    "    # add to the array\n",
    "    reliability_array[lat_ind, lon_ind] = reliability\n",
    "    resilience_array[lat_ind, lon_ind] = resilience\n",
    "    vulnerability_array[lat_ind, lon_ind] = vulnerability\n",
    "\n",
    "# add to the ds\n",
    "ds['reliability'] = (('lat', 'lon'), reliability_array)\n",
    "ds['resilience'] = (('lat', 'lon'), resilience_array)\n",
    "ds['vulnerability'] = (('lat', 'lon'), vulnerability_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sustainability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot reliability, resilience, & vulnerability ---- #\n",
    "\n",
    "plot_dict = {'reliability': 'Reds_r', 'resilience': 'Reds_r', 'vulnerability': 'Reds'}\n",
    "\n",
    "for sus_ind, cmap in plot_dict.items():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # plot huc4 basin as background\n",
    "    gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4].plot(ax=ax, facecolor='whitesmoke', edgecolor='none')\n",
    "\n",
    "    # plot flow lines\n",
    "    max_order = gdf_flow_huc4['StreamOrde'].max()\n",
    "    min_order_to_keep = 4\n",
    "    gdf_flow_huc4.loc[gdf_flow_huc4['StreamOrde']>=min_order_to_keep].plot(ax=ax, linewidth=1, color='tab:blue')\n",
    "\n",
    "    # plot sustainability index\n",
    "    ds[sus_ind].plot(ax=ax, cmap=cmap, vmin=0, vmax=1, alpha=1)\n",
    "\n",
    "    # plot huc4 basin\n",
    "    gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4].plot(ax=ax, facecolor='none', edgecolor='gray')\n",
    "\n",
    "    # plot reservoirs\n",
    "    gdf_reservoirs.loc[gdf_reservoirs['huc4']==huc4].plot(ax=ax, color='tab:gray', marker='v', markersize=80)\n",
    "\n",
    "    plt.savefig(f'{output_dir}/{huc4}/sustainability_index_{sus_ind}.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamflow (Up vs. Downstream) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read simulation results\n",
    "# ds = xr.load_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "\n",
    "# # Omit the first 2 years of spinup\n",
    "# spinup_start = '1988-01-01'\n",
    "# spinup_end = '1990-01-01'\n",
    "# spinup_len = (pd.to_datetime(spinup_end) - pd.to_datetime(spinup_start)).days\n",
    "# # skip the first 2 years of spinup\n",
    "# ds = ds.isel(time=slice(spinup_len, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define functions for calculating drought index & drought characteristics ---- #\n",
    "\n",
    "def cal_ssi_threshold(streamflow_series, quantile=0.25):\n",
    "    \"\"\"\n",
    "    Calculate streamflow drought index based on the threshold method.\n",
    "\n",
    "    streamflow_series: monthly streamflow Series, with datetime index\n",
    "    \"\"\"\n",
    "\n",
    "    streamflow_monthly_mean = streamflow_series.groupby(streamflow_series.index.month).transform(lambda x: x.mean())\n",
    "    streamflow_monthly_threshold = streamflow_series.groupby(streamflow_series.index.month).transform(lambda x: x.quantile(quantile))\n",
    "    ssi = (streamflow_series - streamflow_monthly_threshold) / streamflow_monthly_mean\n",
    "\n",
    "    return(ssi)\n",
    "\n",
    "def cal_ssi_standardized(streamflow_series, mth_accum=3):\n",
    "    \"\"\"\n",
    "    Calculate streamflow drought index based on the standardized method.\n",
    "        Fit a Gamma distribution to the monthly streamflow data, and transform to z score.\n",
    "\n",
    "    streamflow_series: monthly streamflow Series, with datetime index\n",
    "    mon_acc: number of months to accumulate\n",
    "    \"\"\"\n",
    "\n",
    "    # accumulate streamflow\n",
    "    streamflow_series = streamflow_series.rolling(mth_accum).sum()\n",
    "\n",
    "    # fit a Gamma distribution to the monthly streamflow data\n",
    "    alpha, loc, beta = scipy.stats.gamma.fit(streamflow_series.dropna())\n",
    "\n",
    "    # convert to SPI using normal distribution\n",
    "    cdf_gamma = scipy.stats.gamma.cdf(streamflow_series, a=alpha, loc=loc, scale=beta)\n",
    "    ssi = scipy.stats.norm.ppf(cdf_gamma)\n",
    "\n",
    "    return(ssi)\n",
    "\n",
    "def identify_drought_events(drought_index_series, threshold=0):\n",
    "    \"\"\"\n",
    "    Identify all drought events based on the drought index time series.\n",
    "        Drought event: consecutive months with drought index below the threshold\n",
    "\n",
    "    drought_index_series: monthly drought index Series, with datetime index\n",
    "    threshold: the threshold of drought index, below which to be considered as a drought\n",
    "\n",
    "    Return:\n",
    "        drought_event_list: list of drought events, each element is a tuple of (start_date, end_date)\n",
    "    \"\"\"\n",
    "\n",
    "    drought_index_series = drought_index_series.dropna()    # drop NaN values coming from monthly accumulation\n",
    "\n",
    "    # Identifying points where the index is below the threshold\n",
    "    is_drought = drought_index_series < threshold\n",
    "\n",
    "    # Identifying the start of drought events\n",
    "    drought_start = is_drought & (~is_drought.shift(1, fill_value=False))\n",
    "\n",
    "    # Identifying the end of drought events\n",
    "    drought_end = is_drought & (~is_drought.shift(-1, fill_value=False))\n",
    "\n",
    "    # Extracting start and end dates of droughts\n",
    "    start_dates = drought_index_series.index[drought_start]\n",
    "    end_dates = drought_index_series.index[drought_end]\n",
    "\n",
    "    # Creating a list of tuples for each drought event\n",
    "    drought_events = list(zip(start_dates, end_dates))\n",
    "\n",
    "    # Check if it has been a drought at the begining, remove the first event if\n",
    "    if is_drought[0] == True:\n",
    "        drought_events.pop(0)\n",
    "    \n",
    "    # Check if the last drought event is still ongoing, remove the last event if\n",
    "    if is_drought[-1] == True:\n",
    "        drought_events.pop(-1)\n",
    "\n",
    "    return drought_events\n",
    "\n",
    "def cal_drought_duration(drought_event_list):\n",
    "    \"\"\"\n",
    "    Return: list of drought duration for each drought event\n",
    "    \"\"\"\n",
    "    \n",
    "    drought_duration_list = [(end_date - start_date).days // 30 for start_date, end_date in drought_event_list]\n",
    "    # it can come with 0, because I divide by 30, but Feb. has only 28 days\n",
    "    # change 0 to 1\n",
    "    drought_duration_list = [1 if i==0 else i for i in drought_duration_list]\n",
    "\n",
    "    return(drought_duration_list)\n",
    "\n",
    "def cal_drought_severity(drought_index_series, drought_event_list):\n",
    "    \"\"\"\n",
    "    Return: list of drought severity for each drought event\n",
    "    \"\"\"\n",
    "\n",
    "    drought_severity_list = []\n",
    "    for start_date, end_date in drought_event_list:\n",
    "        drought_severity = drought_index_series.loc[start_date:end_date].sum()\n",
    "        drought_severity_list.append(-drought_severity)\n",
    "\n",
    "    return(drought_severity_list)\n",
    "\n",
    "def cal_drought_intensity(drought_index_series, drought_event_list):\n",
    "    \"\"\"\n",
    "    Return: list of drought intensity for each drought event\n",
    "    \"\"\"\n",
    "\n",
    "    drought_intensity_list = []\n",
    "    for start_date, end_date in drought_event_list:\n",
    "        drought_severity = drought_index_series.loc[start_date:end_date].sum()\n",
    "        drought_duration = (end_date - start_date).days // 30\n",
    "        drought_duration = drought_duration if drought_duration > 0 else 1\n",
    "        drought_intensity = drought_severity / drought_duration\n",
    "        drought_intensity_list.append(-drought_intensity)\n",
    "\n",
    "    return(drought_intensity_list)\n",
    "\n",
    "def cal_cdf(data_list):\n",
    "    data_sorted = np.sort(data_list)\n",
    "    cdf = np.arange(1, data_sorted.size + 1) / data_sorted.size\n",
    "    return(data_sorted, cdf)\n",
    "\n",
    "def cal_storage_drought_index(storage_series, quantile=0.25):\n",
    "    \"\"\"\n",
    "    Calculate storage drought index based on the threshold method.\n",
    "\n",
    "    storage_series: monthly storage Series, with datetime index\n",
    "    \"\"\"\n",
    "\n",
    "    storage_monthly_mean = storage_series.groupby(storage_series.index.month).transform(lambda x: x.mean())\n",
    "    storage_monthly_threshold = storage_series.groupby(storage_series.index.month).transform(lambda x: x.quantile(quantile))\n",
    "    sdi = (storage_series - storage_monthly_threshold) / storage_monthly_mean\n",
    "\n",
    "    return(sdi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate drought characteritics dataframe for each huc4 basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate drought characteritics dataframe for each huc4 basin\n",
    "    Saved as a single dataframe\n",
    "\"\"\"\n",
    "\n",
    "# read reservoir metadata for observed storage usage\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx')\n",
    "\n",
    "huc4_list = ['0108', '0110', '0301', '0305', '0306', '0313', '0315', '0505', '0507', '0509', '0511', '0512', '0513', '0601', '0602', '0603', '0701', '0702', '0708', '0710', '0714', '0902', '0804', '1003', '1008', '1012', '1018', '1025', '1026', '1027', '1028', '1029', '1101', '1102', '1107', '1111', '1114', '1201', '1202', '1203', '1210', '1302', '1307', '1401', '1402', '1404', '1506', '1602', '1703', '1704', '1705', '1709', '1711', '1802', '1803', '1804']\n",
    "\n",
    "# initialize the dataframe with columns\n",
    "df_ssi = pd.DataFrame()    # time series of standardized streamflow drought index, columns: {reservoir_id}_ssi_{mth_accum}\n",
    "df_drought_char = pd.DataFrame(columns=['start_date', 'end_date', 'duration', 'intensity', 'severity', 'up_down', 'reservoir_id', 'huc4'])\n",
    "for huc4 in huc4_list:\n",
    "    # read simulation results for each huc4\n",
    "    ds = xr.load_dataset(f'{output_dir}/{huc4}/model_states_{huc4}.nc')\n",
    "\n",
    "    # Omit the first 2 years of spinup\n",
    "    spinup_start = '1988-01-01'\n",
    "    spinup_end = '1990-01-01'\n",
    "    spinup_len = (pd.to_datetime(spinup_end) - pd.to_datetime(spinup_start)).days\n",
    "    # skip the first 2 years of spinup\n",
    "    ds = ds.isel(time=slice(spinup_len, None))\n",
    "\n",
    "    # get the grid indices that locate in the huc4: flow_direction != -1\n",
    "    # & the grid indices that have reservoirs: reservoir_id > 0\n",
    "    res_grid_lat_lon_tup = np.where((ds['flow_direction'] != -1) & (ds['reservoir_id'] > 0))    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "    # loop through each reservoir cell & calculate drought characteristics for each reservoir cell\n",
    "    for lat_ind, lon_ind in zip(res_grid_lat_lon_tup[0], res_grid_lat_lon_tup[1]):\n",
    "        res_gid = ds['reservoir_id'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "\n",
    "        # get the streamflow time series\n",
    "        df_streamflow = ds[['outflow_before_operation', 'outflow_after_operation']].isel(lat=lat_ind, lon=lon_ind).drop_vars(['lat', 'lon']).to_dataframe()\n",
    "        # convert object index to datetime index\n",
    "        df_streamflow.index = pd.to_datetime(df_streamflow.index)\n",
    "\n",
    "        # Calculate streamflow drought index for up- & downstream\n",
    "        # 1. convert to monthly\n",
    "        df_streamflow_monthly = df_streamflow.resample('MS').sum()\n",
    "\n",
    "        # 2. calculate ssi\n",
    "        # threshold-based method\n",
    "        df_streamflow_monthly['outflow_before_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_before_operation'])\n",
    "        df_streamflow_monthly['outflow_after_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_after_operation'])\n",
    "        \n",
    "        # standardized method\n",
    "        mth_accum = 12\n",
    "        # try-except to catch FitError: if the streamflow is too skewed to fit a gamma distribution, which probably comes from failed simulation\n",
    "        try:\n",
    "            df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_before_operation'], mth_accum)\n",
    "            df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_after_operation'], mth_accum)\n",
    "        except Exception as e:\n",
    "            if type(e).__name__ == 'FitError':\n",
    "                print(f'FitError: {res_gid} in {huc4}')\n",
    "                continue\n",
    "        # merge to df_ssi\n",
    "        df_streamflow_monthly_to_merge = df_streamflow_monthly.rename(columns={f'outflow_before_operation_ssi_standardized_{mth_accum}': f'{res_gid}_upstream_ssi_{mth_accum}',\n",
    "                                                                              f'outflow_after_operation_ssi_standardized_{mth_accum}': f'{res_gid}_downstream_ssi_{mth_accum}'}, inplace=False)\n",
    "        df_ssi = pd.concat([df_ssi, df_streamflow_monthly_to_merge[[f'{res_gid}_upstream_ssi_{mth_accum}', f'{res_gid}_downstream_ssi_{mth_accum}']]], axis=1)\n",
    "\n",
    "        # 3. identify drought events\n",
    "        drought_list_upstream = identify_drought_events(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "        drought_list_downstream = identify_drought_events(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "\n",
    "        # drought numbers\n",
    "        n_drought_upstream = len(drought_list_upstream)\n",
    "        n_drought_downstream = len(drought_list_downstream)\n",
    "\n",
    "        # drought duration\n",
    "        drought_duration_upstream_list = cal_drought_duration(drought_list_upstream)\n",
    "        drought_duration_downstream_list = cal_drought_duration(drought_list_downstream)\n",
    "\n",
    "        # drought severity\n",
    "        drought_severity_upstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "        drought_severity_downstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "        # drought intensity\n",
    "        drought_intensity_upstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "        drought_intensity_downstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "        # add to the dataframe, use pd.concat\n",
    "        # add upstream drought \n",
    "        df_drought_char = pd.concat([df_drought_char, pd.DataFrame({'start_date': [start_date for start_date, end_date in drought_list_upstream],\n",
    "                                                                    'end_date': [end_date for start_date, end_date in drought_list_upstream],\n",
    "                                                                    'duration': drought_duration_upstream_list,\n",
    "                                                                    'intensity': drought_intensity_upstream_list,\n",
    "                                                                    'severity': drought_severity_upstream_list,\n",
    "                                                                    'up_down': 'upstream',\n",
    "                                                                    'reservoir_id': res_gid,\n",
    "                                                                    'huc4': huc4})])\n",
    "\n",
    "        # add downstream drought\n",
    "        df_drought_char = pd.concat([df_drought_char, pd.DataFrame({'start_date': [start_date for start_date, end_date in drought_list_downstream],\n",
    "                                                                    'end_date': [end_date for start_date, end_date in drought_list_downstream],\n",
    "                                                                    'duration': drought_duration_downstream_list,\n",
    "                                                                    'intensity': drought_intensity_downstream_list,\n",
    "                                                                    'severity': drought_severity_downstream_list,\n",
    "                                                                    'up_down': 'downstream',\n",
    "                                                                    'reservoir_id': res_gid,\n",
    "                                                                    'huc4': huc4})])\n",
    "\n",
    "        import sys\n",
    "        sys.exit()        \n",
    "        \n",
    "    # delete the ds to save memory\n",
    "    del ds\n",
    "    gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_streamflow_monthly_to_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot for group meeting\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_streamflow_monthly_to_merge['outflow_before_operation'].plot(ax=ax)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Streamflow (acft)')\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/demo_streamflow.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "df_streamflow_monthly_to_merge[f'2193_downstream_ssi_{mth_accum}'].plot()\n",
    "ax.axhline(y=0, color='black', linestyle='--')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel(f'SSI-{mth_accum}')\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/demo_ssi_{mth_accum}.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Save to excel ---- #\n",
    "\n",
    "# df_drought_char.to_excel(f'{output_dir}/drought_characteristics_ssi{mth_accum}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Generate basin-lumped storage drought index & drought characteristics dataframe for each huc4 basin ---- #\n",
    "# ---- # currently use observed storage, and varied threshold to define storage drought ---- #\n",
    "\n",
    "# read reservoir metadata for observed storage usage\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "huc4_list = ['1709', '1705', '1804', '1203']\n",
    "\n",
    "# initialize a dataframe to store basin-level storage time series for all basins, column names are basin huc4\n",
    "df_storage_all_huc4 = pd.DataFrame()\n",
    "\n",
    "# Loop each huc4 basin\n",
    "for huc4 in huc4_list:\n",
    "    # get list of reservoirs in this huc4\n",
    "    res_gid_list = df_res_meta.loc[df_res_meta['huc4']==huc4, 'ID'].tolist()\n",
    "\n",
    "    # Create a dataframe to store storage of each reservoir\n",
    "    df_storage_huc4 = pd.DataFrame()\n",
    "\n",
    "    # Loop through each reservoir, and add all storage together\n",
    "    for res_gid in res_gid_list:\n",
    "        # read storage data and add to basin-level df\n",
    "        reservoir_smax = df_res_meta.loc[df_res_meta['ID']==res_gid, 'Maximum Storage'].values[0]\n",
    "        df_res = pd.read_csv(f'{reservoir_data_dir}/data_training/{res_gid}.csv')\n",
    "        df_res[['Storage', 'NetInflow', 'Release']] = df_res[['Storage', 'NetInflow', 'Release']] * reservoir_smax\n",
    "        df_res['Time'] = pd.to_datetime(df_res['Time'])\n",
    "        df_res.set_index('Time', inplace=True)\n",
    "        # if df_res has duplicate index, drop the duplicate. This can happen when observations have the same date mistakenly.\n",
    "        if df_res.index.duplicated().any():\n",
    "            df_res = df_res[~df_res.index.duplicated(keep='first')]\n",
    "        # drop the first 2 years of spinup if exists\n",
    "        df_res = df_res.loc[df_res.index >= spinup_end]\n",
    "\n",
    "        # add to basin-level df\n",
    "        df_storage_huc4 = pd.concat([df_storage_huc4, df_res['Storage'].rename(f'{res_gid}_storage')], axis=1)\n",
    "\n",
    "    # Post-process the basin-level storage dataframe before adding to all-basin dataframe\n",
    "    # drop NaNs which are at the begining and end\n",
    "    df_storage_huc4['total_storage_temp'] = df_storage_huc4.sum(axis=1, skipna=False)    # this temp sum is created to get the first and last valid index\n",
    "    first_idx = df_storage_huc4['total_storage_temp'].first_valid_index()\n",
    "    last_idx = df_storage_huc4['total_storage_temp'].last_valid_index()\n",
    "    df_storage_huc4 = df_storage_huc4.loc[first_idx:last_idx]\n",
    "    # drop the temp column\n",
    "    df_storage_huc4.drop(columns=['total_storage_temp'], inplace=True)\n",
    "\n",
    "    # change to monthly\n",
    "    df_storage_huc4.index = pd.to_datetime(df_storage_huc4.index)\n",
    "    df_storage_huc4 = df_storage_huc4.resample('MS').mean()\n",
    "\n",
    "    # fill NaN with time period\n",
    "    df_storage_huc4 = df_storage_huc4.interpolate(method='time')\n",
    "\n",
    "    # sum all storage\n",
    "    df_storage_huc4[f'total_storage_{huc4}'] = df_storage_huc4.sum(axis=1, skipna=False)\n",
    "\n",
    "    # Add to all-basin dataframe, using outer join\n",
    "    df_storage_all_huc4 = df_storage_all_huc4.join(df_storage_huc4[f'total_storage_{huc4}'], how='outer')\n",
    "\n",
    "# Calculate storage drought index for each huc4 basin, from df_storage_all_huc4, already monthly\n",
    "for huc4 in huc4_list:\n",
    "    df_storage_all_huc4[f'storage_drought_index_{huc4}'] = cal_storage_drought_index(df_storage_all_huc4[f'total_storage_{huc4}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reservoir storage distributions\n",
    "\n",
    "# # Plot storage distributions for each huc4 basin\n",
    "# for i, huc4 in enumerate(huc4_list):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     df_res_meta.loc[df_res_meta['huc4']==huc4, 'Maximum Storage'].sort_values(ascending=False).plot(kind='bar', ax=ax)\n",
    "#     ax.set_xlabel('Reservoirs')\n",
    "#     ax.set_ylabel('Maximum Storage (acre-feet)')\n",
    "#     ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "#     ax.set_xticks([])\n",
    "#     plt.show()\n",
    "\n",
    "# pie plot of reservoir operation type within each huc4 basin\n",
    "color_map = {\n",
    "        'Hydroelectricity': 'tab:blue',\n",
    "        'Irrigation': 'tab:orange',\n",
    "        'Flood control': 'tab:green',\n",
    "        'Water supply': 'tab:red',\n",
    "        'Recreation': 'tab:purple'\n",
    "    }\n",
    "for huc4 in huc4_list:\n",
    "    print(huc4)\n",
    "    pie_counts = df_res_meta.loc[df_res_meta['huc4']==huc4, 'MAIN_USE'].value_counts()\n",
    "    color_list = [color_map[use] for use in pie_counts.index]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    pie_counts.plot(kind='pie', ax=ax, colors=color_list)\n",
    "    ax.set_ylabel('')\n",
    "    ax.legend(loc='upper left', facecolor='white', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # make background transparent\n",
    "    fig.patch.set_alpha(1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot total storage distribution of all basins\n",
    "df_plot = pd.DataFrame(columns=['total_max_storage'], index=huc4_list)\n",
    "for huc4 in huc4_list:\n",
    "    max_storage_huc4 = df_res_meta.loc[df_res_meta['huc4']==huc4, 'Maximum Storage'].sum()\n",
    "    df_plot.loc[huc4, 'total_max_storage'] = max_storage_huc4\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df_plot.plot(kind='bar', ax=ax, legend=False, color='tab:orange')\n",
    "ax.set_xlabel('HUC4 Basins')\n",
    "ax.set_ylabel('Total Storage Capacity (acre-feet)')\n",
    "ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Generate storage drought index for each single reservoir ---- #\n",
    "# ---- # currently use observed storage, and varied threshold to define storage drought ---- #\n",
    "\n",
    "# read reservoir metadata for observed storage usage\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "huc4_list = ['1709', '1705', '1804', '1203']\n",
    "\n",
    "df_storage_res = pd.DataFrame()    # columns: {reservoir_id}_storage, {reservoir_id}_storage_drought_index\n",
    "# Loop each huc4 basin\n",
    "for huc4 in huc4_list:\n",
    "    # get list of reservoirs in this huc4\n",
    "    res_gid_list = df_res_meta.loc[df_res_meta['huc4']==huc4, 'ID'].tolist()\n",
    "\n",
    "    # Loop through each reservoir, and add all storage together\n",
    "    for res_gid in res_gid_list:\n",
    "        # read storage data and add to basin-level df\n",
    "        reservoir_smax = df_res_meta.loc[df_res_meta['ID']==res_gid, 'Maximum Storage'].values[0]\n",
    "        df_res = pd.read_csv(f'{reservoir_data_dir}/data_training/{res_gid}.csv')\n",
    "        df_res[['Storage', 'NetInflow', 'Release']] = df_res[['Storage', 'NetInflow', 'Release']] * reservoir_smax\n",
    "        df_res['Time'] = pd.to_datetime(df_res['Time'])\n",
    "        df_res.set_index('Time', inplace=True)\n",
    "        # if df_res has duplicate index, drop the duplicate. This can happen when observations have the same date mistakenly.\n",
    "        if df_res.index.duplicated().any():\n",
    "            df_res = df_res[~df_res.index.duplicated(keep='first')]\n",
    "        # drop the first 2 years of spinup if exists\n",
    "        df_res = df_res.loc[df_res.index >= spinup_end]\n",
    "\n",
    "        # add to df_storage_res\n",
    "        df_res.rename(columns={'Storage': f'{res_gid}_storage'}, inplace=True)\n",
    "        df_storage_res = df_storage_res.join(df_res[f'{res_gid}_storage'], how='outer')\n",
    "\n",
    "# reshape to monthly\n",
    "df_storage_res = df_storage_res.resample('MS').mean()\n",
    "# calculate storage drought index\n",
    "for res_gid in df_storage_res.columns.str.split('_').str[0]:\n",
    "    # Remove the first and last NaNs for each reservoir\n",
    "    storage_series = df_storage_res[f'{res_gid}_storage']\n",
    "    first_idx = storage_series.first_valid_index()\n",
    "    last_idx = storage_series.last_valid_index()\n",
    "    storage_series = storage_series.loc[first_idx:last_idx]\n",
    "    \n",
    "    # fill NaN with time period\n",
    "    storage_series = storage_series.interpolate(method='time')\n",
    "\n",
    "    # calculate storage drought index\n",
    "    df_storage_res[f'{res_gid}_storage_drought_index'] = cal_storage_drought_index(storage_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Event-scale Storage Index & Streamflow Drought Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df_storage_res from above \n",
    "\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "for res_gid in df_res_meta.loc[df_res_meta['huc4'].isin(huc4_list), 'ID']:\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot streamflow drought time series\n",
    "    try:\n",
    "        df_ssi[f'{res_gid}_upstream_ssi_{mth_accum}'].plot(ax=ax, color='tab:blue', label='Upstream', linewidth=1)\n",
    "        df_ssi[f'{res_gid}_downstream_ssi_{mth_accum}'].plot(ax=ax, color='tab:orange', label='Downstream', linewidth=1)\n",
    "        ax.legend()\n",
    "\n",
    "        # plot storage drought index time series on twin axis\n",
    "        ax2 = ax.twinx()\n",
    "        df_storage_res[f'{res_gid}_storage_drought_index'].dropna().plot(ax=ax2, color='tab:red', label='Storage', linewidth=1.5)\n",
    "        ax2.legend()\n",
    "\n",
    "    except KeyError:\n",
    "        print(f'{res_gid} not found in df_ssi or df_storage_res')\n",
    "        continue\n",
    "\n",
    "    # make ax and ax2 aligned by setting the same ylim\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax2.set_ylim([-2, 2])\n",
    "\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.savefig(f'{output_dir}/{res_gid}_drought_index.jpg', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Streamflow Drought Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read the results for all huc4 basins ---- #\n",
    "\n",
    "mth_accum = 3\n",
    "df_drought_char = pd.read_excel(f'{output_dir}/drought_characteristics_ssi{mth_accum}.xlsx', dtype={'huc4':str})\n",
    "\n",
    "\n",
    "# read reservoirs to drop from the analysis\n",
    "with open('code/reservoirs_to_drop.txt', 'r') as f:\n",
    "    reservoirs_to_drop = f.read().splitlines()[0]    # only one line\n",
    "reservoirs_to_drop = [int(i) for i in reservoirs_to_drop.split(',')]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot upstream & downstream drought characteristics, by huc4\n",
    "\"\"\"\n",
    "# plot drought duration\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.boxplot(data=df_drought_char, x='huc4', y='duration', hue='up_down', ax=ax, showfliers=False)\n",
    "ax.set_title(f'Drought Duration')\n",
    "ax.set_ylabel('Drought Duration (months)')\n",
    "ax.legend(title='')\n",
    "plt.savefig(f'figures/upstream_downstream_ssi_distribution/drought_duration_ssi{mth_accum}.jpg', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot drought intensity\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.boxplot(data=df_drought_char, x='huc4', y='intensity', hue='up_down', ax=ax, showfliers=False)\n",
    "ax.set_title(f'Drought Intensity')\n",
    "ax.set_ylabel('Drought Intensity')\n",
    "ax.legend(title='')\n",
    "plt.savefig(f'figures/upstream_downstream_ssi_distribution/drought_intensity_ssi{mth_accum}.jpg', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# plot drought severity\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "sns.boxplot(data=df_drought_char, x='huc4', y='severity', hue='up_down', ax=ax, showfliers=False)\n",
    "ax.set_title(f'Drought Severity')\n",
    "ax.set_ylabel('Drought Severity')\n",
    "ax.legend(title='')\n",
    "plt.savefig(f'figures/upstream_downstream_ssi_distribution/drought_severity_ssi{mth_accum}.jpg', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot average change of drought characteristics between upstream and downstream, by each reservoir\n",
    "\"\"\"\n",
    "\n",
    "# read the results for all huc4 basins\n",
    "mth_accum = 3\n",
    "df_drought_char = pd.read_excel(f'{output_dir}/drought_characteristics_ssi{mth_accum}.xlsx', dtype={'huc4':str})\n",
    "\n",
    "# drop reservoirs to drop\n",
    "df_drought_char = df_drought_char[~df_drought_char['reservoir_id'].isin(reservoirs_to_drop)]\n",
    "\n",
    "# Calculate average change of drought characteristics\n",
    "# 1. group by reservoir & upstream/downstream\n",
    "df_drought_char_reservoir_mean = df_drought_char.groupby(['reservoir_id', 'up_down']).mean(numeric_only=True).reset_index()\n",
    "df_drought_char_reservoir_75 = df_drought_char.groupby(['reservoir_id', 'up_down']).quantile(0.75, numeric_only=True).reset_index()\n",
    "df_drought_char_reservoir_25 = df_drought_char.groupby(['reservoir_id', 'up_down']).quantile(0.25, numeric_only=True).reset_index()\n",
    "# add # of drought events for each reservoir\n",
    "df_drought_char_reservoir_mean['n_drought'] = df_drought_char.groupby(['reservoir_id', 'up_down']).size().reset_index(name='n_drought')['n_drought']\n",
    "df_drought_char_reservoir_75['n_drought'] = df_drought_char.groupby(['reservoir_id', 'up_down']).size().reset_index(name='n_drought')['n_drought']\n",
    "df_drought_char_reservoir_25['n_drought'] = df_drought_char.groupby(['reservoir_id', 'up_down']).size().reset_index(name='n_drought')['n_drought']\n",
    "# 2. calculate the change: (downstream - upstream) / upstream, pivot to wide format\n",
    "df_drought_char_reservoir_mean_pivoted = df_drought_char_reservoir_mean.pivot(index='reservoir_id', columns='up_down', values=['n_drought', 'duration', 'intensity', 'severity'])\n",
    "df_drought_char_reservoir_mean_pivoted.columns = [f'{col}_{up_down}' for col, up_down in df_drought_char_reservoir_mean_pivoted.columns]\n",
    "df_drought_char_reservoir_75_pivoted = df_drought_char_reservoir_75.pivot(index='reservoir_id', columns='up_down', values=['n_drought', 'duration', 'intensity', 'severity'])\n",
    "df_drought_char_reservoir_75_pivoted.columns = [f'{col}_{up_down}' for col, up_down in df_drought_char_reservoir_75_pivoted.columns]\n",
    "df_drought_char_reservoir_25_pivoted = df_drought_char_reservoir_25.pivot(index='reservoir_id', columns='up_down', values=['n_drought', 'duration', 'intensity', 'severity'])\n",
    "df_drought_char_reservoir_25_pivoted.columns = [f'{col}_{up_down}' for col, up_down in df_drought_char_reservoir_25_pivoted.columns]\n",
    "\n",
    "for col in ['n_drought', 'duration', 'intensity', 'severity']:\n",
    "    df_drought_char_reservoir_mean_pivoted[f'{col}_change'] = (df_drought_char_reservoir_mean_pivoted[f'{col}_downstream'] - df_drought_char_reservoir_mean_pivoted[f'{col}_upstream']) / df_drought_char_reservoir_mean_pivoted[f'{col}_upstream']\n",
    "    df_drought_char_reservoir_75_pivoted[f'{col}_change'] = (df_drought_char_reservoir_75_pivoted[f'{col}_downstream'] - df_drought_char_reservoir_75_pivoted[f'{col}_upstream']) / df_drought_char_reservoir_75_pivoted[f'{col}_upstream']\n",
    "    df_drought_char_reservoir_25_pivoted[f'{col}_change'] = (df_drought_char_reservoir_25_pivoted[f'{col}_downstream'] - df_drought_char_reservoir_25_pivoted[f'{col}_upstream']) / df_drought_char_reservoir_25_pivoted[f'{col}_upstream']\n",
    "\n",
    "# Drop reservoirs with NaNs\n",
    "df_drought_char_reservoir_mean_pivoted = df_drought_char_reservoir_mean_pivoted.dropna()\n",
    "df_drought_char_reservoir_75_pivoted = df_drought_char_reservoir_75_pivoted.dropna()\n",
    "df_drought_char_reservoir_25_pivoted = df_drought_char_reservoir_25_pivoted.dropna()\n",
    "\n",
    "# Merge with reservoir metadata, for analysis by some attributes\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "df_drought_char_reservoir_mean_pivoted = df_drought_char_reservoir_mean_pivoted.reset_index().merge(df_res_meta[['ID', 'MAIN_USE', 'Maximum Storage', 'huc4']], left_on='reservoir_id', right_on='ID').drop(columns='ID')\n",
    "\n",
    "# ---- Plot ---- #\n",
    "plt.style.use('ggplot')\n",
    "# plot mean change\n",
    "fig, ax = plt.subplots()\n",
    "df_drop_outliers = df_drought_char_reservoir_mean_pivoted.loc[df_drought_char_reservoir_mean_pivoted['n_drought_change'] < 10]\n",
    "sns.boxplot(data=df_drop_outliers[['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']], \n",
    "            ax=ax, showfliers=False)\n",
    "\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.set_ylabel('Reservoir Influence')\n",
    "# change xticklabels 0% to red\n",
    "yticklabels = ax.get_yticklabels()\n",
    "for label in yticklabels:\n",
    "    if label.get_text() == '0%':\n",
    "        label.set_color('tab:blue')\n",
    "        label.set_fontweight('bold')\n",
    "\n",
    "ax.set_xticklabels(['# events', 'Duration', 'Intensity', 'Severity'])\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/drought_characteristics_change_ssi{mth_accum}.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# # plot mean change by reservoir use\n",
    "# for res_use in df_drought_char_reservoir_mean_pivoted['MAIN_USE'].unique():\n",
    "#     fig, ax = plt.subplots()\n",
    "#     sns.boxplot(data=df_drought_char_reservoir_mean_pivoted.loc[df_drought_char_reservoir_mean_pivoted['MAIN_USE']==res_use, ['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']], \n",
    "#                 ax=ax, showfliers=False)\n",
    "\n",
    "#     ax.set_ylim([-1, 1])\n",
    "#     ax.set_xticklabels(['# events', 'Duration', 'Intensity', 'Severity'])\n",
    "#     ax.axhline(y=0, color='tab:gray', linestyle='-', linewidth=1)\n",
    "#     ax.set_title(res_use)\n",
    "#     plt.show()\n",
    "\n",
    "# plot mean change by huc2 region\n",
    "df_drought_char_reservoir_mean_pivoted['huc2'] = df_drought_char_reservoir_mean_pivoted['huc4'].str[0:2]\n",
    "df_drought_char_reservoir_mean_pivoted.sort_values('huc2', inplace=True)\n",
    "for huc2 in df_drought_char_reservoir_mean_pivoted['huc2'].unique():\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.boxplot(data=df_drought_char_reservoir_mean_pivoted.loc[df_drought_char_reservoir_mean_pivoted['huc2']==huc2, ['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']], \n",
    "                ax=ax, showfliers=False)\n",
    "\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    ax.set_ylabel('Reservoir Influence')\n",
    "    # change xticklabels 0% to red\n",
    "    yticklabels = ax.get_yticklabels()\n",
    "    for label in yticklabels:\n",
    "        if label.get_text() == '0%':\n",
    "            label.set_color('tab:blue')\n",
    "            label.set_fontweight('bold')\n",
    "\n",
    "    ax.set_xticklabels(['# events', 'Duration', 'Intensity', 'Severity'])\n",
    "    \n",
    "    plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/drought_characteristics_change_ssi{mth_accum}_{huc2}.jpg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# # plot 75th percentile change\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.boxplot(data=df_drought_char_reservoir_75_pivoted[['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']], \n",
    "#             ax=ax, showfliers=False,\n",
    "#             medianprops=dict(color='black', linewidth=2))\n",
    "\n",
    "# ax.set_xticklabels(['# events', 'Duration', 'Intensity', 'Severity'])\n",
    "# ax.axhline(y=0, color='tab:gray', linestyle='-', linewidth=1)\n",
    "# plt.show()\n",
    "\n",
    "# # plot 25th percentile change\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.boxplot(data=df_drought_char_reservoir_25_pivoted[['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']], \n",
    "#             ax=ax, showfliers=False,\n",
    "#             medianprops=dict(color='black', linewidth=2))\n",
    "\n",
    "# ax.set_xticklabels(['# events', 'Duration', 'Intensity', 'Severity'])\n",
    "# ax.axhline(y=0, color='tab:gray', linestyle='-', linewidth=1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up-Downstream Change Clustering - 2\n",
    "- Use `df_drought_char_reservoir_mean_pivoted`\n",
    "- Seems **NO** meaningful clusters after investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PCA analysis\n",
    "\"\"\"\n",
    "\n",
    "# remove outliers\n",
    "threshold = 2\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers = df_drought_char_reservoir_mean_pivoted.loc[\n",
    "    (df_drought_char_reservoir_mean_pivoted['n_drought_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['duration_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['intensity_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['severity_change'] < threshold)].copy(deep=True)\n",
    "\n",
    "xcols = ['n_drought_change', 'duration_change', 'intensity_change', 'severity_change', 'Maximum Storage']\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers[xcols] = scaler.fit_transform(df_drought_char_reservoir_mean_pivoted_wt_outliers[xcols])\n",
    "\n",
    "# PCA\n",
    "n = 4\n",
    "pca = PCA(n_components=n)\n",
    "pca.fit(df_drought_char_reservoir_mean_pivoted_wt_outliers[xcols])\n",
    "\n",
    "# scatter plot of the first 2 PCs\n",
    "X_transform = pca.transform(df_drought_char_reservoir_mean_pivoted_wt_outliers[xcols])\n",
    "df_pca = pd.DataFrame(X_transform, columns=[f'PC{i+1}' for i in range(n)], index=df_drought_char_reservoir_mean_pivoted_wt_outliers['reservoir_id'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agglomerative Clustering\n",
    "    Cluster the reservoirs\n",
    "\"\"\"\n",
    "\n",
    "# remove outliers\n",
    "threshold = 2\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers = df_drought_char_reservoir_mean_pivoted.loc[\n",
    "    (df_drought_char_reservoir_mean_pivoted['n_drought_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['duration_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['intensity_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['severity_change'] < threshold)].copy(deep=True)\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=5)\n",
    "# clustering = model.fit(df_drought_char_reservoir_mean_pivoted_wt_outliers[['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "KMeans Clustering\n",
    "\"\"\"\n",
    "\n",
    "# remove outliers\n",
    "threshold = 2\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers = df_drought_char_reservoir_mean_pivoted.loc[\n",
    "    (df_drought_char_reservoir_mean_pivoted['n_drought_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['duration_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['intensity_change'] < threshold) &\n",
    "    (df_drought_char_reservoir_mean_pivoted['severity_change'] < threshold)].copy(deep=True)\n",
    "\n",
    "model = KMeans(n_clusters=5)\n",
    "# clustering = model.fit(df_drought_char_reservoir_mean_pivoted_wt_outliers[['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']])\n",
    "clustering = model.fit(df_drought_char_reservoir_mean_pivoted_wt_outliers[['duration_change', 'intensity_change', 'severity_change']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot clusters on two dimensions\n",
    "\"\"\"\n",
    "\n",
    "# add cluster labels to df_drought_char_reservoir_mean_pivoted\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers['cluster'] = clustering.labels_\n",
    "\n",
    "# plot clusters on 2D\n",
    "for x in ['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']:\n",
    "    for y in ['n_drought_change', 'duration_change', 'intensity_change', 'severity_change']:\n",
    "        if x != y:\n",
    "            fig, ax = plt.subplots()\n",
    "            sns.scatterplot(data=df_drought_char_reservoir_mean_pivoted_wt_outliers,\n",
    "                            x=x, y=y, hue=clustering.labels_, ax=ax, palette='tab10', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the clusters on map\n",
    "\"\"\"\n",
    "\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "# add cluster labels to df_drought_char_reservoir_mean_pivoted\n",
    "df_drought_char_reservoir_mean_pivoted_wt_outliers['cluster'] = clustering.labels_\n",
    "\n",
    "# merge with reservoir metadata\n",
    "df_drought_char_clustered = df_drought_char_reservoir_mean_pivoted_wt_outliers.merge(df_res_meta[['ID', 'lattitude', 'longtitude']], left_on='reservoir_id', right_on='ID').drop(columns='ID')\n",
    "\n",
    "# convert to geodataframe\n",
    "gdf_drought_char_clustered = gpd.GeoDataFrame(df_drought_char_clustered, geometry=gpd.points_from_xy(df_drought_char_clustered['longtitude'], df_drought_char_clustered['lattitude']))\n",
    "gdf_drought_char_clustered.set_crs(crs, inplace=True)\n",
    "\n",
    "# Concat huc4 gdf of each huc2\n",
    "gdf_huc4_conus = gpd.GeoDataFrame()\n",
    "gdf_huc2_conus = gpd.GeoDataFrame()\n",
    "for huc2 in huc2_conus:\n",
    "    # read HUC2s\n",
    "    gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "    gdf_huc4_in_huc2 = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "    gdf_huc2 = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "\n",
    "    # add to the conus gdf\n",
    "    gdf_huc4_conus = pd.concat([gdf_huc4_conus, gdf_huc4_in_huc2], ignore_index=True)\n",
    "    gdf_huc2_conus = pd.concat([gdf_huc2_conus, gdf_huc2], ignore_index=True)\n",
    "\n",
    "# ---- Plot ---- #\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# plot huc2\n",
    "gdf_huc2_conus.plot(ax=ax, facecolor='whitesmoke', edgecolor='tab:gray', linewidth=1)\n",
    "\n",
    "gdf_drought_char_clustered.plot(ax=ax, column='cluster', cmap='Dark2', legend=False, markersize=50, alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot streamflow drought characteristics (cumulative density) ---- #\n",
    "\n",
    "# get the grid indices that locate in the huc4: flow_direction != -1\n",
    "# & the grid indices that have reservoirs: reservoir_id > 0\n",
    "res_grid_lat_lon_tup = np.where((ds['flow_direction'] != -1) & (ds['reservoir_id'] > 0))    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "# loop through each reservoir cell & do the analysis\n",
    "for lat_ind, lon_ind in zip(res_grid_lat_lon_tup[0], res_grid_lat_lon_tup[1]):\n",
    "    res_gid = ds['reservoir_id'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "    print(res_gid)\n",
    "\n",
    "    # get streamflow time series\n",
    "    df_streamflow = ds[['outflow_before_operation', 'outflow_after_operation']].isel(lat=lat_ind, lon=lon_ind).drop_vars(['lat', 'lon']).to_dataframe()\n",
    "    # convert object index to datetime index\n",
    "    df_streamflow.index = pd.to_datetime(df_streamflow.index)\n",
    "\n",
    "    # Calculate streamflow drought index for up- & downstream\n",
    "    # 1. convert to monthly\n",
    "    df_streamflow_monthly = df_streamflow.resample('MS').sum()\n",
    "\n",
    "    # 2. calculate ssi\n",
    "    df_streamflow_monthly['outflow_before_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_before_operation'])\n",
    "    df_streamflow_monthly['outflow_after_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_after_operation'])\n",
    "    \n",
    "    mth_accum = 3\n",
    "    df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_before_operation'], mth_accum)\n",
    "    df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_after_operation'], mth_accum)\n",
    "\n",
    "    # Analyze drought events\n",
    "    drought_list_upstream = identify_drought_events(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "    drought_list_downstream = identify_drought_events(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "\n",
    "    # drought numbers\n",
    "    n_drought_upstream = len(drought_list_upstream)\n",
    "    n_drought_downstream = len(drought_list_downstream)\n",
    "\n",
    "    # drought duration\n",
    "    drought_duration_upstream_list = cal_drought_duration(drought_list_upstream)\n",
    "    drought_duration_downstream_list = cal_drought_duration(drought_list_downstream)\n",
    "\n",
    "    # drought severity\n",
    "    drought_severity_upstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "    drought_severity_downstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "    # drought intensity\n",
    "    drought_intensity_upstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "    drought_intensity_downstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "    # Plot cdf\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n",
    "\n",
    "    # duration\n",
    "    data_sorted, cdf = cal_cdf(drought_duration_upstream_list)\n",
    "    ax[0].plot(data_sorted, cdf, label='upstream')\n",
    "    data_sorted, cdf = cal_cdf(drought_duration_downstream_list)\n",
    "    ax[0].plot(data_sorted, cdf, label='downstream')\n",
    "    ax[0].set_xlabel('Drought duration (months)')\n",
    "    ax[0].set_ylabel('Cumulative probability')\n",
    "\n",
    "    # severity\n",
    "    data_sorted, cdf = cal_cdf(drought_severity_upstream_list)\n",
    "    ax[1].plot(data_sorted, cdf, label='upstream')\n",
    "    data_sorted, cdf = cal_cdf(drought_severity_downstream_list)\n",
    "    ax[1].plot(data_sorted, cdf, label='downstream')\n",
    "    ax[1].set_xlabel('Drought severity (unitless)')\n",
    "    ax[1].set_ylabel('Cumulative probability')\n",
    "\n",
    "    # intensity\n",
    "    data_sorted, cdf = cal_cdf(drought_intensity_upstream_list)\n",
    "    ax[2].plot(data_sorted, cdf, label='upstream')\n",
    "    data_sorted, cdf = cal_cdf(drought_intensity_downstream_list)\n",
    "    ax[2].plot(data_sorted, cdf, label='downstream')\n",
    "    ax[2].set_xlabel('Drought intensity (unitless)')\n",
    "    ax[2].set_ylabel('Cumulative probability')\n",
    "\n",
    "    ax[0].legend()\n",
    "\n",
    "    # plt.savefig(f'{output_dir}/{huc4}/drought_cdf_{res_gid}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot streamflow drought characteristics ---- #\n",
    "\n",
    "# get the grid indices that locate in the huc4: flow_direction != -1\n",
    "# & the grid indices that have reservoirs: reservoir_id > 0\n",
    "res_grid_lat_lon_tup = np.where((ds['flow_direction'] != -1) & (ds['reservoir_id'] > 0))    # (lat_ind_array, lon_ind_array) pair\n",
    "\n",
    "# read reservoir storage observation\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx')\n",
    "\n",
    "# loop through each reservoir cell & do the analysis\n",
    "for lat_ind, lon_ind in zip(res_grid_lat_lon_tup[0], res_grid_lat_lon_tup[1]):\n",
    "    res_gid = ds['reservoir_id'].isel(lat=lat_ind, lon=lon_ind).values\n",
    "    print(res_gid)\n",
    "\n",
    "    # get streamflow time series\n",
    "    df_streamflow = ds[['outflow_before_operation', 'outflow_after_operation']].isel(lat=lat_ind, lon=lon_ind).drop_vars(['lat', 'lon']).to_dataframe()\n",
    "    # convert object index to datetime index\n",
    "    df_streamflow.index = pd.to_datetime(df_streamflow.index)\n",
    "\n",
    "    # read reservoir storage time series (currently, use observed storage)\n",
    "    # TODO: use simulated storage, after the storage assimilation is implemented\n",
    "    reservoir_smax = df_res_meta.loc[df_res_meta['ID']==res_gid, 'Maximum Storage'].values[0]\n",
    "    df_ts = pd.read_csv(f'{reservoir_data_dir}/data_training/{res_gid}.csv')\n",
    "    df_ts[['Storage', 'NetInflow', 'Release']] = df_ts[['Storage', 'NetInflow', 'Release']] * reservoir_smax\n",
    "    df_ts['Time'] = pd.to_datetime(df_ts['Time'])\n",
    "    df_ts.set_index('Time', inplace=True)\n",
    "    # if df_ts has duplicate index, drop the duplicate. This can happen when observations have the same date mistakenly.\n",
    "    if df_ts.index.duplicated().any():\n",
    "        df_ts = df_ts[~df_ts.index.duplicated(keep='first')]\n",
    "\n",
    "    # merge streamflow & storage, based on time index\n",
    "    df_streamflow = df_streamflow.merge(df_ts[['Storage']], how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Calculate streamflow drought index for up- & downstream\n",
    "    # 1. convert to monthly\n",
    "    df_streamflow_monthly = df_streamflow.resample('MS').sum()\n",
    "\n",
    "    # 2. calculate ssi\n",
    "    df_streamflow_monthly['outflow_before_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_before_operation'])\n",
    "    df_streamflow_monthly['outflow_after_operation_ssi_threshold'] = cal_ssi_threshold(df_streamflow_monthly['outflow_after_operation'])\n",
    "    \n",
    "    mth_accum = 12\n",
    "    df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_before_operation'], mth_accum)\n",
    "    df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'] = cal_ssi_standardized(df_streamflow_monthly['outflow_after_operation'], mth_accum)\n",
    "\n",
    "    # plot to visualize streamflow drought index & storage\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.plot(df_streamflow_monthly.index, df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], label='Upstream', color='tab:blue')\n",
    "    ax.plot(df_streamflow_monthly.index, df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], label='Downstream', color='tab:orange')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df_streamflow_monthly.index, df_streamflow_monthly['Storage'], label='Storage', color='tab:red', linestyle='--')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # # Analyze drought events\n",
    "    # drought_list_upstream = identify_drought_events(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "    # drought_list_downstream = identify_drought_events(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], threshold=0)\n",
    "\n",
    "    # # drought numbers\n",
    "    # n_drought_upstream = len(drought_list_upstream)\n",
    "    # n_drought_downstream = len(drought_list_downstream)\n",
    "\n",
    "    # # drought duration\n",
    "    # drought_duration_upstream_list = cal_drought_duration(drought_list_upstream)\n",
    "    # drought_duration_downstream_list = cal_drought_duration(drought_list_downstream)\n",
    "\n",
    "    # # drought severity\n",
    "    # drought_severity_upstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "    # drought_severity_downstream_list = cal_drought_severity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "    # # drought intensity\n",
    "    # drought_intensity_upstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_before_operation_ssi_standardized_{mth_accum}'], drought_list_upstream)\n",
    "    # drought_intensity_downstream_list = cal_drought_intensity(df_streamflow_monthly[f'outflow_after_operation_ssi_standardized_{mth_accum}'], drought_list_downstream)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/donghui/Box Sync/Research/PhD/Projects/Water_Supply_Drought/code/effective_velocity.txt', 'r') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "huc4_good_list = [line[0:4] for line in lines if line[6:10]!='xxxx']\n",
    "huc4_bad_list = [line[0:4] for line in lines if line[6:10]=='xxxx']\n",
    "\n",
    "print(huc4_good_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "huc4_gdrom_list = df_res_meta['huc4'].unique()\n",
    "huc4_highlight_list = ['0108', '0110', '0305', '0306', '0313', '0315', '0505', '0507', '0509', '0511', '0512', '0513', '0601', '0602', '0603', '0701', '0714', '0902', '0804', '1003', '1008', '1012', '1018', '1029', '1101', '1102', '1107', '1114', '1203', '1302', '1401', '1402', '1404', '1602', '1703', '1704', '1705', '1709', '1711', '1802', '1803', '1804']\n",
    "\n",
    "huc4_highlight_list = huc4_good_list\n",
    "\n",
    "# Concat huc4 gdf of each huc2\n",
    "gdf_huc4_conus = gpd.GeoDataFrame()\n",
    "gdf_huc2_conus = gpd.GeoDataFrame()\n",
    "for huc2 in huc2_conus:\n",
    "    # read HUC2s\n",
    "    gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "    gdf_huc4_in_huc2 = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "    gdf_huc2 = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "\n",
    "    # add to the conus gdf\n",
    "    gdf_huc4_conus = pd.concat([gdf_huc4_conus, gdf_huc4_in_huc2], ignore_index=True)\n",
    "    gdf_huc2_conus = pd.concat([gdf_huc2_conus, gdf_huc2], ignore_index=True)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# plot huc4 across conus\n",
    "gdf_huc4_conus.plot(ax=ax, facecolor='whitesmoke', edgecolor='gray', linewidth=1)\n",
    "\n",
    "# # highlight GDROM-included basins (452 reservoirs)\n",
    "# gdf_huc4_conus.loc[gdf_huc4_conus['huc4'].isin(huc4_gdrom_list)].plot(ax=ax, facecolor='tab:blue', edgecolor='gray', linewidth=1)\n",
    "# # add annotation for the highlighted basins\n",
    "# for idx, row in gdf_huc4_conus.loc[gdf_huc4_conus['huc4'].isin(huc4_gdrom_list)].iterrows():\n",
    "#     ax.text(row.geometry.centroid.x, row.geometry.centroid.y, row['huc4'], fontsize=8, ha='center', va='center')\n",
    "\n",
    "# highlight the huc4 basins (selected for analysis)\n",
    "gdf_huc4_conus.loc[gdf_huc4_conus['huc4'].isin(huc4_highlight_list)].plot(ax=ax, facecolor='tab:blue', edgecolor='gray', linewidth=1)\n",
    "    \n",
    "# gdf_huc4_conus.loc[gdf_huc4_conus['huc4'].isin(huc4_good_list)].plot(ax=ax, facecolor='tab:red', edgecolor='gray', linewidth=1)\n",
    "# gdf_huc4_conus.loc[gdf_huc4_conus['huc4'].isin(huc4_bad_list)].plot(ax=ax, facecolor='tab:orange', edgecolor='gray', linewidth=1)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/huc4_conus.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot for a given huc2\n",
    "\"\"\"\n",
    "\n",
    "df_res_meta = pd.read_excel(f'{data_dir}/processed/reservoir_metadata.xlsx', dtype={'huc4':str})\n",
    "\n",
    "huc4_gdrom_list = df_res_meta['huc4'].unique()\n",
    "huc4_highlight_list = huc4_good_list\n",
    "huc2_highlight = '17'\n",
    "\n",
    "# Concat huc4 gdf of each huc2\n",
    "gdf_huc4_conus = gpd.GeoDataFrame()\n",
    "gdf_huc2_conus = gpd.GeoDataFrame()\n",
    "for huc2 in huc2_conus:\n",
    "    # read HUC2s\n",
    "    gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "    gdf_huc4_in_huc2 = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "    gdf_huc2 = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "\n",
    "    # add to the conus gdf\n",
    "    gdf_huc4_conus = pd.concat([gdf_huc4_conus, gdf_huc4_in_huc2], ignore_index=True)\n",
    "    gdf_huc2_conus = pd.concat([gdf_huc2_conus, gdf_huc2], ignore_index=True)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# plot huc2 across conus\n",
    "gdf_huc2_conus.plot(ax=ax, facecolor='whitesmoke', edgecolor='gray', linewidth=1)\n",
    "\n",
    "# highlight the huc2\n",
    "gdf_huc4_conus.loc[(gdf_huc4_conus['huc4'].str.startswith(huc2_highlight))].plot(ax=ax, facecolor='tab:blue', edgecolor='gray', linewidth=1, alpha=0.2)\n",
    "\n",
    "# highlight the huc4 basins (selected for analysis)\n",
    "gdf_huc4_conus.loc[(gdf_huc4_conus['huc4'].isin(huc4_highlight_list)) & \n",
    "                   (gdf_huc4_conus['huc4'].str.startswith(huc2_highlight))].plot(ax=ax, facecolor='tab:blue', edgecolor='gray', linewidth=1)\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.savefig(f'/Users/donghui/Box Sync/UIUC/Group Meeting/2024-SPRING/my_presentation/figures/huc4_conus_{huc2_highlight}.jpg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
