{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Water_Supply_Drought'\n",
    "data_dir = f'{base_dir}/data'\n",
    "\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .nc for CONUS Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read DTR global data & NLDAS runoff data to generate domain file ---- #\n",
    "\n",
    "# file paths\n",
    "flow_dir_file = f'{data_dir}/raw/DRT_Hydrography/upscaled_global_hydrography/by_HydroSHEDS_Hydro1k/flow_direction/DRT_8th_FDR_globe.asc'\n",
    "runoff_file = f'{data_dir}/processed/nldas_daily/20000101.nc'    # an arbitrary daily runoff file\n",
    "new_domain_file = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "\n",
    "# Read flow direction data\n",
    "with open(flow_dir_file) as f:\n",
    "    # read header\n",
    "    ncols = int(f.readline().split()[1])\n",
    "    nrows = int(f.readline().split()[1])\n",
    "    xllcorner = float(f.readline().split()[1])\n",
    "    yllcorner = float(f.readline().split()[1])\n",
    "    cellsize = float(f.readline().split()[1])\n",
    "    nodata_value = float(f.readline().split()[1])\n",
    "    # read flow direction data\n",
    "    flow_dir_array = np.loadtxt(f, dtype=int)\n",
    "\n",
    "# Read runoff data & get the border of the domain\n",
    "with nc.Dataset(runoff_file) as ds_nldas:\n",
    "    lon_nldas = ds_nldas.variables['lon'][:]    # index increase -> lon increase\n",
    "    lat_nldas = ds_nldas.variables['lat'][:]    # index increase -> lat increase\n",
    "    min_lon = lon_nldas.min()\n",
    "    max_lon = lon_nldas.max()\n",
    "    min_lat = lat_nldas.min()\n",
    "    max_lat = lat_nldas.max()\n",
    "    \n",
    "# Subset flow direction data to the domain\n",
    "# get the row and column indices of the domain in the flow direction array\n",
    "row_start = int((yllcorner + nrows * cellsize - max_lat) / cellsize)\n",
    "row_end = int((yllcorner + nrows * cellsize - min_lat) / cellsize)\n",
    "col_start = int((min_lon - xllcorner) / cellsize)\n",
    "col_end = int((max_lon - xllcorner) / cellsize)\n",
    "# subset flow direction array using the row and column indices\n",
    "flow_dir_array_domain = flow_dir_array[row_start:row_end+1, col_start:col_end+1]\n",
    "flow_dir_array_domain = np.flipud(flow_dir_array_domain)    # flip the array upside down to match the latitudes, becasue the subset is from south to north (down to up)\n",
    "\n",
    "# # Create a new netCDF file and write the subset flow direction data to it\n",
    "# with nc.Dataset(new_domain_file, 'w', format='NETCDF4') as ds_domain:\n",
    "#     # create dimensions\n",
    "#     ds_domain.createDimension('lat', flow_dir_array_domain.shape[0])\n",
    "#     ds_domain.createDimension('lon', flow_dir_array_domain.shape[1])\n",
    "\n",
    "#     # create variables\n",
    "#     lat = ds_domain.createVariable('lat', 'f4', ('lat',))\n",
    "#     lon = ds_domain.createVariable('lon', 'f4', ('lon',))\n",
    "#     flow_dir = ds_domain.createVariable('flow_dir', 'i4', ('lat', 'lon',))\n",
    "#     # add an ID variable for each grid cell\n",
    "#     id = ds_domain.createVariable('id', 'i4', ('lat', 'lon',))    # 224 * 464; lat * lon\n",
    "\n",
    "#     # write data\n",
    "#     lat[:] = lat_nldas\n",
    "#     lon[:] = lon_nldas\n",
    "#     flow_dir[:, :] = flow_dir_array_domain\n",
    "#     id[:, :] = np.arange(1, flow_dir_array_domain.size+1).reshape(flow_dir_array_domain.shape)\n",
    "\n",
    "#     # set attributes\n",
    "#     flow_dir.long_name = 'D8 Flow direction'\n",
    "#     flow_dir.missing_value = nodata_value\n",
    "#     id.long_name = 'Grid ID for the CONUS NLDAS grid'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Grid-based Flow Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the flow map, using arrows ---- #\n",
    "\n",
    "def d8_to_uv(d8_value):\n",
    "    \"\"\"\n",
    "    Convert the D8 flow direction value to u and v components.\n",
    "    The values follow the pattern (1, 2, 4, 8, 16, 32, 64, 128),\n",
    "    representing the following directions (assuming north is up):\n",
    "    32  64 128\n",
    "    16  x   1\n",
    "    8   4   2\n",
    "\n",
    "    :param d8_value: D8 flow direction value [int]\n",
    "    \"\"\"\n",
    "    \n",
    "    directions = {\n",
    "        1: (1, 0),    # right (east)\n",
    "        2: (1, -1),   # down-right (southeast)\n",
    "        4: (0, -1),   # down (south)\n",
    "        8: (-1, -1),  # down-left (southwest)\n",
    "        16: (-1, 0),  # left (west)\n",
    "        32: (-1, 1),  # up-left (northwest)\n",
    "        64: (0, 1),   # up (north)\n",
    "        128: (1, 1)   # up-right (northeast)\n",
    "    }\n",
    "    return directions.get(d8_value, (0, 0))\n",
    "\n",
    "# load conus domain data\n",
    "with nc.Dataset(new_domain_file) as ds_domain:\n",
    "    lat_array = ds_domain.variables['lat'][:].data\n",
    "    lon_array = ds_domain.variables['lon'][:].data\n",
    "    flow_dir_array = ds_domain.variables['flow_dir'][:, :].data\n",
    "\n",
    "# Generate a meshgrid for the latitude and longitude\n",
    "lon_grid, lat_grid = np.meshgrid(lon_array, lat_array)\n",
    "\n",
    "# Convert the flow direction values to u and v components\n",
    "u = np.zeros_like(flow_dir_array, dtype=int)\n",
    "v = np.zeros_like(flow_dir_array, dtype=int)\n",
    "for i in range(flow_dir_array.shape[0]):\n",
    "    for j in range(flow_dir_array.shape[1]):\n",
    "        u[i, j], v[i, j] = d8_to_uv(flow_dir_array[i, j])\n",
    "\n",
    "# Plot the flow direction\n",
    "fig, ax = plt.subplots()\n",
    "ax.quiver(lon_grid, lat_grid, u, v)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the flow map, using heatmap style ---- #\n",
    "\n",
    "# load conus domain data\n",
    "with nc.Dataset(new_domain_file) as ds_domain:\n",
    "    lat_array = ds_domain.variables['lat'][:].data\n",
    "    lon_array = ds_domain.variables['lon'][:].data\n",
    "    flow_dir_array = ds_domain.variables['flow_dir'][:, :].data\n",
    "\n",
    "# create a boolean array to indicate whether a cell is a along the flow path\n",
    "flow_path_array = np.isin(flow_dir_array, [1, 2, 4, 8, 16, 32, 64, 128])\n",
    "\n",
    "# generate a meshgrid for the latitude and longitude\n",
    "lon_grid, lat_grid = np.meshgrid(lon_array, lat_array)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(lon_grid, lat_grid, flow_path_array, cmap='binary')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected River Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def get_grids_in_hu(lon_array, lat_array, gdf_huc):\n",
    "    \"\"\"\n",
    "    Get grids (lon-lat) within the target HU\n",
    "    \n",
    "    gdf_huc: geodataframe of the target HU\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create lon-lat pairs from global vars: lon_array and lat_array\n",
    "    lon_lat_all = [[Point(lon, lat), i, j] for i, lon in enumerate(lon_array) for j, lat in enumerate(lat_array)]    # [Point(lon, lat), i-lon index, j-lat index]\n",
    "    \n",
    "    # index the wanted hu from the gdf\n",
    "    huc_geo = gdf_huc['geometry'].values   # the polygon for this huc\n",
    "        \n",
    "    # find (lon, lat) pairs within the area\n",
    "    lon_lat_sub = [i for i in lon_lat_all if huc_geo.contains(i[0])[0]]\n",
    "\n",
    "    # create point geodataframe for selected points and check\n",
    "    d = {'lon index': [i[1] for i in lon_lat_sub], 'lat index': [i[2] for i in lon_lat_sub]}\n",
    "    gdf_points = gpd.GeoDataFrame(d, \n",
    "                                  geometry=[i[0] for i in lon_lat_sub], crs='EPSG:4326')   # lon index, lat index, geometry\n",
    "    \n",
    "    result = {\n",
    "        'grids_in_hu': gdf_points,    # gdf - lon index, lat index, geometry; the index represents index in .nc files\n",
    "        'others': (lon_array, lat_array, gdf_huc)    # this is mainly for plot check\n",
    "    }\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def d8_to_uv(d8_value):\n",
    "    \"\"\"\n",
    "    Convert the D8 flow direction value to u and v components.\n",
    "    The values follow the pattern (1, 2, 4, 8, 16, 32, 64, 128),\n",
    "    representing the following directions (assuming north is up):\n",
    "    32  64 128\n",
    "    16  x   1\n",
    "    8   4   2\n",
    "\n",
    "    :param d8_value: D8 flow direction value [int]\n",
    "    \"\"\"\n",
    "    \n",
    "    directions = {\n",
    "        1: (1, 0),    # right (east)\n",
    "        2: (1, -1),   # down-right (southeast)\n",
    "        4: (0, -1),   # down (south)\n",
    "        8: (-1, -1),  # down-left (southwest)\n",
    "        16: (-1, 0),  # left (west)\n",
    "        32: (-1, 1),  # up-left (northwest)\n",
    "        64: (0, 1),   # up (north)\n",
    "        128: (1, 1)   # up-right (northeast)\n",
    "    }\n",
    "\n",
    "    return directions.get(d8_value, (0, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read files ---- #\n",
    "\n",
    "huc4 = '1804'\n",
    "\n",
    "nhd_data_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Drought_Cycle_Analysis/Data'\n",
    "crs = 'EPSG:4326'\n",
    "huc2_conus = [f'0{i}' if i<10 else f'{i}' for i in range(1, 19)]\n",
    "\n",
    "# read HUCs\n",
    "huc2 = huc4[0:2]\n",
    "gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "gdf_huc2_all = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "gdf_huc4_all = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "gdf_huc6_all = gpd.read_file(gdb_file, layer='WBDHU6')\n",
    "gdf_huc8_all = gpd.read_file(gdb_file, layer='WBDHU8')\n",
    "gdf_huc10_all = gpd.read_file(gdb_file, layer='WBDHU10')\n",
    "\n",
    "# set crs\n",
    "gdf_huc2_all = gdf_huc2_all.set_crs(crs, inplace=False, allow_override=True)    # includes the huc2 region\n",
    "gdf_huc4_all = gdf_huc4_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc4 subregions in this huc2 region\n",
    "gdf_huc6_all = gdf_huc6_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc6 basins in this huc2 region\n",
    "gdf_huc8_all = gdf_huc8_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc8 subbasins in this huc2 region\n",
    "gdf_huc10_all = gdf_huc10_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc10 subbasins in this huc2 region\n",
    "\n",
    "########## Prepare flow lines ##########\n",
    "\n",
    "if huc2 == '03':    # multiple NHDP files for 03\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['N','S','W']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['N','S','W']]\n",
    "elif huc2 == '10': \n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['U','L']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['U','L']]\n",
    "else:\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDPlusAttributes']\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDSnapshot/Hydrography']\n",
    "\n",
    "gdf_flow_list = []\n",
    "for flow_attr_file, hydro_file in zip(flow_attr_file_list, hydro_file_list):\n",
    "    gdf_fline_vaa = gpd.read_file(flow_attr_file, layer='PlusFlowlineVAA')\n",
    "    gdf_fline = gpd.read_file(hydro_file, layer='NHDFlowline')\n",
    "\n",
    "    # change COMID to ComID if the error exists\n",
    "    if not 'ComID' in gdf_fline:\n",
    "        gdf_fline.rename(columns={'COMID':'ComID'}, inplace=True)\n",
    "\n",
    "    # change vaa file ComID to int\n",
    "    to_int_var = ['ComID', 'StreamOrde', 'StreamCalc']\n",
    "    gdf_fline_vaa[to_int_var] = gdf_fline_vaa[to_int_var].astype(int)\n",
    "\n",
    "    # merge this two gdfs\n",
    "    to_merge_vars = ['ComID', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode']\n",
    "    gdf_flow = gdf_fline.merge(gdf_fline_vaa[to_merge_vars], how='inner', on='ComID')\n",
    "    \n",
    "    gdf_flow_list.append(gdf_flow)\n",
    "\n",
    "gdf_flow = pd.concat(gdf_flow_list)\n",
    "\n",
    "# set crs\n",
    "gdf_flow = gdf_flow.set_crs(crs, inplace=True, allow_override=True)\n",
    "\n",
    "# subset to the target huc4\n",
    "gdf_flow_huc4 = gdf_flow.sjoin(gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4], how='inner', predicate='intersects')\n",
    "\n",
    "########## End Prepare flow lines ##########\n",
    "\n",
    "# read conus domain\n",
    "new_domain_file = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "with nc.Dataset(new_domain_file) as ds_domain:\n",
    "    lat_array = ds_domain.variables['lat'][:].data\n",
    "    lon_array = ds_domain.variables['lon'][:].data\n",
    "    flow_dir_array = ds_domain.variables['flow_dir'][:, :].data\n",
    "    grid_id_conus_array = ds_domain.variables['id'][:, :].data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the flow map, using arrows ---- #\n",
    "\n",
    "# Get the grids within the target HU\n",
    "gdf_huc4 = gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4]\n",
    "result = get_grids_in_hu(lon_array, lat_array, gdf_huc4)\n",
    "gdf_grid_index_in_hu = result['grids_in_hu']\n",
    "lon_index = gdf_grid_index_in_hu['lon index'].values    # contains duplicate values\n",
    "lat_index = gdf_grid_index_in_hu['lat index'].values    # contains duplicate values\n",
    "\n",
    "# Subset flow direction array to the HU\n",
    "# technically, I didn't \"subset\", just set the flow direction values outside the HU to -9999\n",
    "mask = np.ones_like(flow_dir_array, dtype=bool)\n",
    "mask[lat_index, lon_index] = False\n",
    "flow_dir_array[mask] = -9999\n",
    "# subset the array\n",
    "lat_index_unique = np.unique(lat_index)\n",
    "lon_index_unique = np.unique(lon_index)\n",
    "xx, yy = np.meshgrid(lat_index_unique, lon_index_unique, indexing='ij')\n",
    "flow_dir_array_huc4 = flow_dir_array[xx, yy]    # get the square matrix of flow direction\n",
    "\n",
    "# Generate a meshgrid for the latitude and longitude\n",
    "lon_array_huc4 = np.unique(lon_array[lon_index])\n",
    "lat_array_huc4 = np.unique(lat_array[lat_index])\n",
    "lon_grid, lat_grid = np.meshgrid(lon_array_huc4, lat_array_huc4)\n",
    "\n",
    "# Convert the flow direction values to u and v components\n",
    "u = np.zeros_like(flow_dir_array_huc4, dtype=int)\n",
    "v = np.zeros_like(flow_dir_array_huc4, dtype=int)\n",
    "for i in range(flow_dir_array_huc4.shape[0]):\n",
    "    for j in range(flow_dir_array_huc4.shape[1]):\n",
    "        u[i, j], v[i, j] = d8_to_uv(flow_dir_array_huc4[i, j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the flow direction\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# plot flow line\n",
    "# specify to which stream order\n",
    "max_order = gdf_flow_huc4['StreamOrde'].max()\n",
    "min_order_to_keep = 4\n",
    "gdf_flow_huc4.loc[gdf_flow_huc4['StreamOrde']>=min_order_to_keep].plot(ax=ax, linewidth=1)\n",
    "\n",
    "ax.quiver(lon_grid, lat_grid, u, v, scale=40)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "gdf_huc4.plot(ax=ax, facecolor='none', edgecolor='k', linewidth=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .nc for Reservoirs CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_grid(lat, lon, lat_array, lon_array):\n",
    "    \"\"\"\n",
    "    Get the nearest grid cell index for a given lat-lon pair.\n",
    "    \n",
    "    lat: latitude of the reservoir\n",
    "    lon: longitude of the reservoir\n",
    "    lat_array: latitude array of the domain\n",
    "    lon_array: longitude array of the domain\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    dist_min = 999999\n",
    "    # get the index of the nearest grid cell\n",
    "    # gosh, this is so slow...\n",
    "    # maybe I should use geopandas distance or geopy distance for vectorization\n",
    "    for i, grid_lat in enumerate(lat_array):\n",
    "        for j, grid_lon in enumerate(lon_array):\n",
    "            dist = geopy.distance.distance((lat, lon), (grid_lat, grid_lon))\n",
    "            if dist < dist_min:\n",
    "                dist_min = dist\n",
    "                lat_index = i\n",
    "                lon_index = j\n",
    "    \n",
    "    return((lat_index, lon_index))\n",
    "\n",
    "# Read reservoir information\n",
    "reservoir_file = f'{data_dir}/processed/reservoir_metadata.xlsx'\n",
    "df_reservoir = pd.read_excel(reservoir_file, dtype={'huc4':str})\n",
    "\n",
    "# Read conus domain\n",
    "domain_file = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "with nc.Dataset(domain_file) as ds_domain:\n",
    "    id_array = ds_domain.variables['id'][:, :].data\n",
    "    lat_array = ds_domain.variables['lat'][:].data\n",
    "    lon_array = ds_domain.variables['lon'][:].data\n",
    "\n",
    "# Find the grid cell where the reservoir is located, i.e., nearest grid\n",
    "reservoir_lat_on_id_array = np.zeros((df_reservoir.shape[0], 3), dtype=int)\n",
    "for index, row in df_reservoir.iterrows():\n",
    "    lat = row['lattitude']\n",
    "    lon = row['longtitude']\n",
    "    lat_index, lon_index = get_nearest_grid(lat, lon, lat_array, lon_array)\n",
    "\n",
    "    reservoir_lat_on_id_array[index, 0] = row['ID']\n",
    "    reservoir_lat_on_id_array[index, 1] = lat_index\n",
    "    reservoir_lat_on_id_array[index, 2] = lon_index\n",
    "\n",
    "# Create new nc to store the reservoir information & grid cell ID (to be continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new nc to store the reservoir information & grid cell ID (continued)\n",
    "\n",
    "reservoir_nc = f'{data_dir}/processed/LRR/input/reservoirs.nc'\n",
    "\n",
    "with nc.Dataset(reservoir_nc, 'w', format='NETCDF4') as ds_reservoir:\n",
    "    # create dimensions\n",
    "    ds_reservoir.createDimension('index', df_reservoir.shape[0])\n",
    "\n",
    "    # create variables\n",
    "    id = ds_reservoir.createVariable('gid', 'i4', ('index',))    # HydroShare ID: GRAND_ID + self-defined ID\n",
    "    name = ds_reservoir.createVariable('name', 'S1', ('index',))    # dam name\n",
    "    main_use = ds_reservoir.createVariable('main_use', 'S1', ('index',))    # main use of the reservoir\n",
    "    grid_id = ds_reservoir.createVariable('grid_id', 'i4', ('index',))\n",
    "    lat = ds_reservoir.createVariable('lat', 'f4', ('index',))\n",
    "    lon = ds_reservoir.createVariable('lon', 'f4', ('index',))\n",
    "    max_storage = ds_reservoir.createVariable('max_storage', 'f4', ('index',))\n",
    "\n",
    "    # write data\n",
    "    id[:] = reservoir_lat_on_id_array[:, 0]\n",
    "    name[:] = df_reservoir['Dam name '].values\n",
    "    main_use[:] = df_reservoir['MAIN_USE'].values\n",
    "    grid_id[:] = id_array[reservoir_lat_on_id_array[:, 1], reservoir_lat_on_id_array[:, 2]]\n",
    "    lat[:] = df_reservoir['lattitude'].values\n",
    "    lon[:] = df_reservoir['longtitude'].values\n",
    "    max_storage[:] = df_reservoir['Maximum Storage'].values\n",
    "\n",
    "    # set attributes\n",
    "    id.long_name = 'Reservoir ID in the GRDOM HydroShare dataset'\n",
    "    name.long_name = 'Dam name'\n",
    "    main_use.long_name = 'Main use of the reservoir'\n",
    "    grid_id.long_name = 'ID of the grid cell where the reservoir is located'\n",
    "    lat.long_name = 'Latitude of the reservoir'\n",
    "    lon.long_name = 'Longitude of the reservoir'\n",
    "    max_storage.long_name = 'Maximum storage of the reservoir (in acre-feet)'\n",
    "    max_storage.units = 'acre-feet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the reservoirs & grid match on the domain map ---- #\n",
    "\n",
    "conus_domain_nc = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "reservoir_nc = f'{data_dir}/processed/LRR/input/reservoirs.nc'\n",
    "\n",
    "with nc.Dataset(reservoir_nc) as ds:\n",
    "    lat = ds.variables['lat'][:].data\n",
    "    lon = ds.variables['lon'][:].data\n",
    "    grid_id_res = ds.variables['grid_id'][:].data\n",
    "\n",
    "with nc.Dataset(conus_domain_nc) as ds:\n",
    "    lat_domain = ds.variables['lat'][:].data\n",
    "    lon_domain = ds.variables['lon'][:].data\n",
    "    grid_id_domain = ds.variables['id'][:, :].data\n",
    "\n",
    "# For each item in grid_id_res, find the corresponding index in grid_id_domain\n",
    "grid_id_domain_lat_index = np.zeros_like(grid_id_res, dtype=int)\n",
    "grid_id_domain_lon_index = np.zeros_like(grid_id_res, dtype=int)\n",
    "for i, grid_id in enumerate(grid_id_res):\n",
    "    grid_id_domain_lat_index[i] = np.argwhere(grid_id_domain==grid_id)[0][0]\n",
    "    grid_id_domain_lon_index[i] = np.argwhere(grid_id_domain==grid_id)[0][1]\n",
    "    \n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(lon, lat, s=10, alpha=0.5, label='Reservoirs')\n",
    "ax.scatter(lon_domain[grid_id_domain_lon_index], lat_domain[grid_id_domain_lat_index], s=50, alpha=0.5, label='Grid cells')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each item in grid_id_res, find the corresponding index in grid_id_domain\n",
    "grid_id_domain_lat_index = np.zeros_like(grid_id_res, dtype=int)\n",
    "grid_id_domain_lon_index = np.zeros_like(grid_id_res, dtype=int)\n",
    "for i, grid_id in enumerate(grid_id_res):\n",
    "    grid_id_domain_lat_index[i] = np.argwhere(grid_id_domain==grid_id)[0][0]\n",
    "    grid_id_domain_lon_index[i] = np.argwhere(grid_id_domain==grid_id)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Water Demand for CONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read NLDAS runoff data & see its format\n",
    "\n",
    "nldas_nc = f'{data_dir}/processed/LRR/input/nldas_runoff.nc'\n",
    "\n",
    "ds_nldas = xr.load_dataset(nldas_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_nldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Process global demand data from GCAM ---- #\n",
    "# ---- Crop data ---- #\n",
    "\n",
    "def pop_2010_to_1990(df):\n",
    "    \"\"\"\n",
    "    Populate 2010 profile to 1990-2009\n",
    "        In reality, I pop to 1988, since the simulation needs 2 years of spin-up time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of new column names from 199001 to 200912\n",
    "    new_columns = [f\"{year}{month:02d}\" for year in range(1988, 2010) for month in range(1, 13)]\n",
    "\n",
    "    # Extract the 2010 data for mapping\n",
    "    df_2010 = df[[f\"2010{month:02d}\" for month in range(1, 13)]]\n",
    "\n",
    "    # Initialize a dictionary to hold new column data\n",
    "    new_columns_dict = {}\n",
    "\n",
    "    # Populate the dictionary with new columns and their corresponding 2010 values\n",
    "    for new_column in new_columns:\n",
    "        month = int(new_column[-2:])  # Extract the month part\n",
    "        corresponding_2010_column = f\"2010{month:02d}\"\n",
    "        new_columns_dict[new_column] = df[corresponding_2010_column].values\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    new_columns_df = pd.DataFrame(new_columns_dict)\n",
    "\n",
    "    # Concatenate this new DataFrame with the original one\n",
    "    # First, select the non-monthly columns from the original DataFrame\n",
    "    non_monthly_cols = df[['Grid_ID', 'lon', 'lat', 'ilon', 'ilat']]\n",
    "\n",
    "    # Then concatenate the non-monthly columns, new monthly columns, and original monthly columns\n",
    "    concatenated_df = pd.concat([non_monthly_cols, new_columns_df, df.drop(non_monthly_cols.columns, axis=1)], axis=1)\n",
    "\n",
    "    return(concatenated_df)\n",
    "\n",
    "def csv2ds(df):\n",
    "    \"\"\"Convert the GCAM csv file to an xarray Dataset\"\"\"\n",
    "\n",
    "    # Parsing the date columns\n",
    "    date_columns = [col for col in df.columns if col.isdigit()]\n",
    "\n",
    "    # Melt the DataFrame\n",
    "    df_melted = df.melt(id_vars=['lon', 'lat'], value_vars=date_columns, var_name='time', value_name=var_name)\n",
    "    df_melted['time'] = pd.to_datetime(df_melted['time'], format='%Y%m')\n",
    "    # sort by lon and lat\n",
    "    df_melted.sort_values(by=['lon', 'lat', 'time'], inplace=True)\n",
    "\n",
    "    # set index\n",
    "    df_melted.set_index(['lon', 'lat', 'time'], inplace=True, drop=True)\n",
    "\n",
    "    # create a xarray Dataset from df_melted\n",
    "    # Please NOTE: there are many NaNs in the dataset, because the xarray Dataset creates a value for every possible combination of lon, lat, and time, \n",
    "    # even if the value does not exist in the original dataset.\n",
    "    ds = xr.Dataset.from_dataframe(df_melted)\n",
    "\n",
    "    return(ds)\n",
    "    \n",
    "\n",
    "dir_path = f'{data_dir}/raw/GCAM_demand_data/ssp1_rcp45_gfdl_consumption_crops_monthly_cropped'\n",
    "var_name_pos = 2\n",
    "# dir_path = f'{data_dir}/raw/GCAM_demand_data/ssp1_rcp45_gfdl_consumption_sectors_monthly_cropped'\n",
    "# var_name_pos = 0\n",
    "\n",
    "file_name_list = [f for f in os.listdir(dir_path) if f.endswith('.csv')]\n",
    "\n",
    "# Process for the 1st file\n",
    "file_name = file_name_list[0]\n",
    "file_path = f'{dir_path}/{file_name}'\n",
    "var_name = file_name.split('_')[var_name_pos]\n",
    "print(var_name)\n",
    "# read data\n",
    "df = pd.read_csv(file_path)\n",
    "# populate 2010 profile to 1990-2009\n",
    "df_popped = pop_2010_to_1990(df)\n",
    "ds = csv2ds(df_popped)\n",
    "\n",
    "# Process for the rest of the files\n",
    "for file_name in file_name_list[1:]:\n",
    "    file_path = f'{dir_path}/{file_name}'\n",
    "    var_name = file_name.split('_')[var_name_pos]\n",
    "    print(var_name)\n",
    "    # read data\n",
    "    df = pd.read_csv(file_path)\n",
    "    # populate 2010 profile to 1990-2009\n",
    "    df_popped = pop_2010_to_1990(df)\n",
    "    ds_new = csv2ds(df_popped)\n",
    "    # merge with the previous ds\n",
    "    ds = xr.merge([ds, ds_new])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Upscale to the CONUS grid ---- #\n",
    "\n",
    "# Read conus domain\n",
    "conus_domain_nc = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "ds_conus = xr.load_dataset(conus_domain_nc)\n",
    "\n",
    "# Upscale to the conus grid\n",
    "# 1. crop the global data to the conus domain\n",
    "ds_conus_crop = ds.sel(lon=slice(ds_conus['lon'].min(), ds_conus['lon'].max()), lat=slice(ds_conus['lat'].min(), ds_conus['lat'].max()))\n",
    "\n",
    "# 2. upscale to the conus grid\n",
    "# before that, we need to convert the km3 per grid to depth per grid\n",
    "ds_conus_crop_depth = ds_conus_crop.copy()\n",
    "for var in ds_conus_crop_depth.data_vars:\n",
    "    ds_conus_crop_depth[var] = ds_conus_crop_depth[var] / (55*55) * 10e6    # km3 -> mm\n",
    "    ds_conus_crop_depth[var].attrs['units'] = 'mm/month'\n",
    "\n",
    "ds_conus_upscale = ds_conus_crop_depth.interp(lat=ds_conus['lat'], lon=ds_conus['lon'], method='nearest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- save to netcdf ---- #\n",
    "# ds_conus_upscale.to_netcdf(f'{data_dir}/processed/GCAM_demand_data/crop_conus.nc')\n",
    "# ds_conus_upscale.to_netcdf(f'{data_dir}/processed/GCAM_demand_data/sector_conus.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Add all the sectors together ---- #\n",
    "# ---- new code: only add sector together ---- #\n",
    "\n",
    "ds_sector_conus = xr.load_dataset(f'{data_dir}/processed/GCAM_demand_data/sector_conus.nc')\n",
    "\n",
    "# Add all the sectors together\n",
    "# create a new dataset with the same dimensions as ds_crop_conus\n",
    "ds_all_conus = xr.Dataset(\n",
    "    data_vars={'total_consumption': (['lon', 'lat', 'time'], np.zeros_like(ds_sector_conus['tcdirr']))},\n",
    "    coords={'lat': ds_sector_conus['lat'], 'lon': ds_sector_conus['lon'], 'time': ds_sector_conus['time']})\n",
    "\n",
    "# add all the sectors together\n",
    "for var in ds_sector_conus.data_vars:\n",
    "    ds_all_conus['total_consumption'] += ds_sector_conus[var]\n",
    "\n",
    "# fill all the NaNs with 0\n",
    "ds_all_conus_filled = ds_all_conus.fillna(0)\n",
    "\n",
    "# transpose to reorder the dimensions\n",
    "ds_all_conus_filled = ds_all_conus_filled.transpose('time', 'lat', 'lon')\n",
    "\n",
    "# Interpolate the data to daily scale\n",
    "# assume constant consumption within a month\n",
    "start_date = ds_all_conus_filled['time'].min().values\n",
    "end_date = ds_all_conus_filled['time'].max().values\n",
    "daily_time = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "ds_all_conus_daily = ds_all_conus_filled.reindex(time=daily_time, method='ffill')\n",
    "\n",
    "# divide by 30 to get mm/day\n",
    "ds_all_conus_daily = ds_all_conus_daily / 30\n",
    "\n",
    "# add units\n",
    "ds_all_conus_daily['total_consumption'].attrs['units'] = 'mm/day'\n",
    "\n",
    "# change ds_all_conus_daily time to string yyyy-mm-dd\n",
    "ds_all_conus_daily['time'] = ds_all_conus_daily['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to netcdf\n",
    "# ds_all_conus_daily.to_netcdf(f'{data_dir}/processed/LRR/input/total_consumption_conus.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- Add all the sectors together ---- #\n",
    "# # ---- old code: add crop and sector together ---- #\n",
    "\n",
    "# ds_crop_conus = xr.load_dataset(f'{data_dir}/processed/GCAM_demand_data/crop_conus.nc')\n",
    "# ds_sector_conus = xr.load_dataset(f'{data_dir}/processed/GCAM_demand_data/sector_conus.nc')\n",
    "\n",
    "# # Add all the sectors together\n",
    "# # create a new dataset with the same dimensions as ds_crop_conus\n",
    "# ds_all_conus = xr.Dataset(\n",
    "#     data_vars={'total_consumption': (['lon', 'lat', 'time'], np.zeros_like(ds_crop_conus['Wheat']))},\n",
    "#     coords={'lat': ds_crop_conus['lat'], 'lon': ds_crop_conus['lon'], 'time': ds_crop_conus['time']})\n",
    "\n",
    "# # add all the sectors together: crops\n",
    "# for var in ds_crop_conus.data_vars:\n",
    "#     ds_all_conus['total_consumption'] += ds_crop_conus[var]\n",
    "\n",
    "# # add all the sectors together: other sectors\n",
    "# for var in ds_sector_conus.data_vars:\n",
    "#     ds_all_conus['total_consumption'] += ds_sector_conus[var]\n",
    "\n",
    "# # fill all the NaNs with 0\n",
    "# ds_all_conus_filled = ds_all_conus.fillna(0)\n",
    "\n",
    "# # transpose to reorder the dimensions\n",
    "# ds_all_conus_filled = ds_all_conus_filled.transpose('time', 'lat', 'lon')\n",
    "\n",
    "# # Interpolate the data to daily scale\n",
    "# # assume constant consumption within a month\n",
    "# start_date = ds_all_conus_filled['time'].min().values\n",
    "# end_date = ds_all_conus_filled['time'].max().values\n",
    "# daily_time = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# ds_all_conus_daily = ds_all_conus_filled.reindex(time=daily_time, method='ffill')\n",
    "\n",
    "# # divide by 30 to get mm/day\n",
    "# ds_all_conus_daily = ds_all_conus_daily / 30\n",
    "\n",
    "# # add units\n",
    "# ds_all_conus_daily['total_consumption'].attrs['units'] = 'mm/day'\n",
    "\n",
    "# # change ds_all_conus_daily time to string yyyy-mm-dd\n",
    "# ds_all_conus_daily['time'] = ds_all_conus_daily['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
