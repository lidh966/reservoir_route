{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Water_Supply_Drought'\n",
    "data_dir = f'{base_dir}/data'\n",
    "\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Grids & Sort by Upstream-Downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Read geo files related to a given HUC4 basin ---- #\n",
    "\n",
    "huc4 = '0601'\n",
    "\n",
    "nhd_data_dir = '/Users/donghui/Box Sync/Research/PhD/Projects/Drought_Cycle_Analysis/Data'\n",
    "crs = 'EPSG:4326'\n",
    "huc2_conus = [f'0{i}' if i<10 else f'{i}' for i in range(1, 19)]\n",
    "\n",
    "# read HUCs\n",
    "huc2 = huc4[0:2]\n",
    "gdb_file = f'{nhd_data_dir}/Raw/WBD/WBD_{huc2}_HU2_GDB.gdb'\n",
    "gdf_huc2_all = gpd.read_file(gdb_file, layer='WBDHU2')\n",
    "gdf_huc4_all = gpd.read_file(gdb_file, layer='WBDHU4')\n",
    "gdf_huc6_all = gpd.read_file(gdb_file, layer='WBDHU6')\n",
    "gdf_huc8_all = gpd.read_file(gdb_file, layer='WBDHU8')\n",
    "gdf_huc10_all = gpd.read_file(gdb_file, layer='WBDHU10')\n",
    "\n",
    "# set crs\n",
    "gdf_huc2_all = gdf_huc2_all.set_crs(crs, inplace=False, allow_override=True)    # includes the huc2 region\n",
    "gdf_huc4_all = gdf_huc4_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc4 subregions in this huc2 region\n",
    "gdf_huc6_all = gdf_huc6_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc6 basins in this huc2 region\n",
    "gdf_huc8_all = gdf_huc8_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc8 subbasins in this huc2 region\n",
    "gdf_huc10_all = gdf_huc10_all.set_crs(crs, inplace=False, allow_override=True)    # includes all huc10 subbasins in this huc2 region\n",
    "\n",
    "########## Prepare flow lines ##########\n",
    "\n",
    "if huc2 == '03':    # multiple NHDP files for 03\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['N','S','W']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['N','S','W']]\n",
    "elif huc2 == '10': \n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDPlusAttributes' for i in ['U','L']]\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}{i}/NHDSnapshot/Hydrography' for i in ['U','L']]\n",
    "else:\n",
    "    flow_attr_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDPlusAttributes']\n",
    "    hydro_file_list = [f'{nhd_data_dir}/Raw/NHDPlus/NHDPlus{huc2}/NHDSnapshot/Hydrography']\n",
    "\n",
    "gdf_flow_list = []\n",
    "for flow_attr_file, hydro_file in zip(flow_attr_file_list, hydro_file_list):\n",
    "    gdf_fline_vaa = gpd.read_file(flow_attr_file, layer='PlusFlowlineVAA')\n",
    "    gdf_fline = gpd.read_file(hydro_file, layer='NHDFlowline')\n",
    "\n",
    "    # change COMID to ComID if the error exists\n",
    "    if not 'ComID' in gdf_fline:\n",
    "        gdf_fline.rename(columns={'COMID':'ComID'}, inplace=True)\n",
    "\n",
    "    # change vaa file ComID to int\n",
    "    to_int_var = ['ComID', 'StreamOrde', 'StreamCalc']\n",
    "    gdf_fline_vaa[to_int_var] = gdf_fline_vaa[to_int_var].astype(int)\n",
    "\n",
    "    # merge this two gdfs\n",
    "    to_merge_vars = ['ComID', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode']\n",
    "    gdf_flow = gdf_fline.merge(gdf_fline_vaa[to_merge_vars], how='inner', on='ComID')\n",
    "    \n",
    "    gdf_flow_list.append(gdf_flow)\n",
    "\n",
    "gdf_flow = pd.concat(gdf_flow_list)\n",
    "\n",
    "# set crs\n",
    "gdf_flow = gdf_flow.set_crs(crs, inplace=True, allow_override=True)\n",
    "\n",
    "# subset to the target huc4\n",
    "gdf_flow_huc4 = gdf_flow.sjoin(gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4], how='inner', predicate='intersects')\n",
    "\n",
    "########## End Prepare flow lines ##########\n",
    "\n",
    "\n",
    "########## Read .nc files ##########\n",
    "conus_grid_nc = f'{data_dir}/processed/LRR/input/conus_nldas_grid.nc'\n",
    "conus_reservoir_nc = f'{data_dir}/processed/LRR/input/reservoirs.nc'\n",
    "# Read CONUS grids\n",
    "with nc.Dataset(conus_grid_nc) as conus_grid:\n",
    "    lon_array = conus_grid.variables['lon'][:]\n",
    "    lat_array = conus_grid.variables['lat'][:]\n",
    "    grid_id_array = conus_grid.variables['id'][:, :]\n",
    "    flow_dir_array = conus_grid.variables['flow_dir'][:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/donghui/Box Sync/Research/PhD/Projects/Drought_Cycle_Analysis/Data'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhd_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "def get_grids_in_hu(lon_array, lat_array, gdf_huc):\n",
    "    \"\"\"\n",
    "    Get grids (lon-lat) within the target HU\n",
    "    \n",
    "    gdf_huc: geodataframe of the target HU\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create lon-lat pairs from global vars: lon_array and lat_array\n",
    "    lon_lat_all = [[Point(lon, lat), i, j] for i, lon in enumerate(lon_array) for j, lat in enumerate(lat_array)]    # [Point(lon, lat), i-lon index, j-lat index]\n",
    "    \n",
    "    # index the wanted hu from the gdf\n",
    "    huc_geo = gdf_huc['geometry'].values   # the polygon for this huc\n",
    "        \n",
    "    # find (lon, lat) pairs within the area\n",
    "    lon_lat_sub = [i for i in lon_lat_all if huc_geo.contains(i[0])[0]]\n",
    "\n",
    "    # create point geodataframe for selected points and check\n",
    "    d = {'lon index': [i[1] for i in lon_lat_sub], 'lat index': [i[2] for i in lon_lat_sub]}\n",
    "    gdf_points = gpd.GeoDataFrame(d, \n",
    "                                  geometry=[i[0] for i in lon_lat_sub], crs='EPSG:4326')   # lon index, lat index, geometry\n",
    "    \n",
    "    result = {\n",
    "        'grids_in_hu': gdf_points,    # gdf - lon index, lat index, geometry; the index represents index in .nc files\n",
    "        'others': (lon_array, lat_array, gdf_huc)    # this is mainly for plot check\n",
    "    }\n",
    "    \n",
    "    return(result)\n",
    "\n",
    "def get_downstream_cell(i, j, direction):\n",
    "    \"\"\"\n",
    "    Returns the downstream cell coordinates based on D8 flow direction.\n",
    "    d8 directions:\n",
    "    32  64 128\n",
    "    16  x   1\n",
    "    8   4   2\n",
    "\n",
    "    Parameters:\n",
    "    i (int): Row index of the current cell\n",
    "    j (int): Column index of the current cell\n",
    "    direction (int): D8 flow direction code of the current cell\n",
    "\n",
    "    Returns:\n",
    "    tuple: Coordinates (row, col) of the downstream cell, or None if no downstream\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the changes in row (di) and column (dj) for each flow direction\n",
    "    # PLEASE note: the direction is defined as such, because the latitude array increases as the row index increases & longitude array increases as the col index increases!!!\n",
    "\n",
    "    direction_map = {\n",
    "        1: (0, 1),     # East: i, j+1\n",
    "        2: (-1, 1),    # Southeast: i-1, j+1\n",
    "        4: (-1, 0),    # South: i-1, j (move south, decrease row index)\n",
    "        8: (-1, -1),   # Southwest: i-1, j-1\n",
    "        16: (0, -1),   # West: i, j-1\n",
    "        32: (1, -1),   # Northwest: i+1, j-1\n",
    "        64: (1, 0),    # North: i+1, j (move north, increase row index)\n",
    "        128: (1, 1)    # Northeast: i+1, j+1\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get the changes in row and column for the given direction\n",
    "    di, dj = direction_map.get(direction, (0, 0))    # if direction is not in the direction_map, return (0, 0)\n",
    "\n",
    "    # If di and dj are both 0, it means the direction is not defined (e.g., a sink)\n",
    "    if di == 0 and dj == 0:\n",
    "        return None\n",
    "\n",
    "    # Calculate the coordinates of the downstream cell\n",
    "    downstream_i = i + di\n",
    "    downstream_j = j + dj\n",
    "    \n",
    "    return (downstream_i, downstream_j)\n",
    "\n",
    "# test\n",
    "get_downstream_cell(2, 16, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the grids within the target HU\n",
    "gdf_huc4 = gdf_huc4_all.loc[gdf_huc4_all['huc4']==huc4]\n",
    "gdf_grid_index_in_hu = get_grids_in_hu(lon_array, lat_array, gdf_huc4)['grids_in_hu']\n",
    "lon_index = gdf_grid_index_in_hu['lon index'].values    # contains duplicate values\n",
    "lat_index = gdf_grid_index_in_hu['lat index'].values    # contains duplicate values\n",
    "\n",
    "# Subset the flow direction array to the target HU basin (the rectangular area covering the HUC4 basin)\n",
    "# technically, I didn't \"subset\", just set the flow direction values outside the HU to -9999\n",
    "# first, set all conus grids outside the target HU to -9999\n",
    "mask = np.ones_like(flow_dir_array, dtype=bool)\n",
    "mask[lat_index, lon_index] = False\n",
    "flow_dir_array[mask] = -1    # set all conus grids outside the target HU to -1\n",
    "# second, subset the flow direction array to the target HU basin\n",
    "lat_index_unique = np.unique(lat_index)\n",
    "lon_index_unique = np.unique(lon_index)\n",
    "flow_dir_array_huc4 = flow_dir_array[np.ix_(lat_index_unique, lon_index_unique)]\n",
    "\n",
    "# also, get the grid id array for the target HU basin for record\n",
    "grid_id_array_huc4 = grid_id_array[np.ix_(lat_index_unique, lon_index_unique)]\n",
    "lat_array_huc4 = lat_array[lat_index_unique]\n",
    "lon_array_huc4 = lon_array[lon_index_unique]\n",
    "\n",
    "########## Sort the grids ##########\n",
    "\n",
    "# create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# add nodes\n",
    "nrows, ncols = flow_dir_array_huc4.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        # add node for each grid\n",
    "        # if the node flows out of the huc4 basin, skip\n",
    "        if flow_dir_array_huc4[i, j] == -1:\n",
    "            continue\n",
    "        \n",
    "        G.add_node((i, j), \n",
    "                   flow_dir=flow_dir_array_huc4[i, j], grid_id=grid_id_array_huc4[i, j], grid_lat=lat_array_huc4[i], grid_lon=lon_array_huc4[j])\n",
    "\n",
    "        # determine downstream grids and add edges\n",
    "        downstream_grid_ij = get_downstream_cell(i, j, flow_dir_array_huc4[i, j])\n",
    "\n",
    "        if downstream_grid_ij is not None and flow_dir_array_huc4[downstream_grid_ij] != -1:\n",
    "            # if downstream grid is not None AND the downstream grid is not outside the huc4 basin\n",
    "            G.add_edge((i, j), downstream_grid_ij)\n",
    "\n",
    "# typological sorting\n",
    "sorted_grid_list = list(nx.topological_sort(G))\n",
    "\n",
    "# store the sorted grid id and upstream grid id for each grid: {grid: [upstream grid list]}\n",
    "# the grid order is following the topological sorting\n",
    "upstream_grid_dict = {grid: [] for grid in sorted_grid_list}\n",
    "for grid in sorted_grid_list:\n",
    "    upstream_grid_dict[grid] = list(G.predecessors(grid))\n",
    "# convert the grid index in upstream_grid_dict to grid id (the grid attribute in the node)\n",
    "upstream_grid_id_dict = {G.nodes[grid]['grid_id']: [G.nodes[upstream_grid]['grid_id'] for upstream_grid in upstream_grid_dict[grid]] for grid in upstream_grid_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): [],\n",
       " (0, 1): [],\n",
       " (1, 1): [],\n",
       " (1, 0): [(0, 0), (0, 1)],\n",
       " (2, 0): [(1, 0), (1, 1)]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "nodes = [(0,0), (0,1), (1,0), (2,0), (1,1)]\n",
    "G.add_nodes_from(nodes)\n",
    "edges = [((0,0), (1,0)), ((0,1), (1,0)), ((1,0), (2,0)), ((1,1), (2,0))]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# typological sorting\n",
    "sorted_grid_list = list(nx.topological_sort(G))\n",
    "\n",
    "# store the sorted grid id and upstream grid id for each grid: {grid: [upstream grid list]}\n",
    "# the grid order is following the topological sorting\n",
    "upstream_grid_dict = {grid: [] for grid in sorted_grid_list}\n",
    "for grid in sorted_grid_list:\n",
    "    upstream_grid_dict[grid] = list(G.predecessors(grid))\n",
    "\n",
    "upstream_grid_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualize the graph on the map ---- #\n",
    "\n",
    "nodes_data = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "geometry = [Point(xy) for xy in zip(nodes_data['grid_lon'], nodes_data['grid_lat'])]\n",
    "gdf_nodes = gpd.GeoDataFrame(nodes_data, geometry=geometry, crs='EPSG:4326')\n",
    "\n",
    "\n",
    "# ------------------- Plot ------------------- #\n",
    "# Variables to store components of arrows\n",
    "x = []\n",
    "y = []\n",
    "dx = []\n",
    "dy = []\n",
    "\n",
    "default_lon = lon_array_huc4[0]\n",
    "default_lat = lat_array_huc4[0]\n",
    "for edge in G.edges():\n",
    "    start_node = G.nodes[edge[0]]\n",
    "    end_node = G.nodes[edge[1]]\n",
    "    try:\n",
    "        x_start, y_start = start_node['grid_lon'], start_node['grid_lat']\n",
    "        x_end, y_end = end_node['grid_lon'], end_node['grid_lat']\n",
    "    except KeyError:\n",
    "        x_start, y_start = default_lon, default_lat\n",
    "        x_end, y_end = default_lon, default_lat\n",
    "\n",
    "    x.append(x_start)\n",
    "    y.append(y_start)\n",
    "    dx.append(x_end - x_start)\n",
    "    dy.append(y_end - y_start)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot arrows\n",
    "ax.quiver(x, y, dx, dy, angles='xy', scale_units='xy', scale=1, color='gray')\n",
    "\n",
    "# Plot nodes\n",
    "gdf_nodes.plot(ax=ax, marker='o', color='black', alpha=0.7, markersize=10)\n",
    "\n",
    "gdf_huc4.plot(ax=ax, facecolor='none', edgecolor='tab:gray', linewidth=1)\n",
    "\n",
    "# plot flow line\n",
    "# specify to which stream order\n",
    "max_order = gdf_flow_huc4['StreamOrde'].max()\n",
    "min_order_to_keep = 4\n",
    "gdf_flow_huc4.loc[gdf_flow_huc4['StreamOrde']>=min_order_to_keep].plot(ax=ax, linewidth=1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, '')],\n",
       "       [(0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, '')],\n",
       "       [(0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, ''),\n",
       "        (0, 0., 0., 0., 0., 0., 0., False, '')]],\n",
       "      dtype=[('grid_id', '<i8'), ('elevation', '<f8'), ('slope', '<f8'), ('runoff', '<f8'), ('inflow', '<f8'), ('outflow', '<f8'), ('storage', '<f8'), ('has_reservoir', '?'), ('reservoir_id', '<U10')])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = np.zeros((3, 5), dtype=[\n",
    "    ('var_1', int),\n",
    "    ('var_2', float),\n",
    "    ('var_3', str)\n",
    "])\n",
    "\n",
    "grid = np.zeros((3, 5), dtype=[('grid_id', int), ('elevation', float), ('slope', float), ('runoff', float), ('inflow', float), ('outflow', float), ('storage', float), ('has_reservoir', bool), ('reservoir_id', 'U10')])\n",
    "\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Daily NLDAS Runoff .nc to Single Input File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1980-01-01 00:00:00\n",
      "Processing 1980-04-10 00:00:00\n",
      "Processing 1980-07-19 00:00:00\n",
      "Processing 1980-10-27 00:00:00\n",
      "Processing 1981-02-04 00:00:00\n",
      "Processing 1981-05-15 00:00:00\n",
      "Processing 1981-08-23 00:00:00\n",
      "Processing 1981-12-01 00:00:00\n",
      "Processing 1982-03-11 00:00:00\n",
      "Processing 1982-06-19 00:00:00\n",
      "Processing 1982-09-27 00:00:00\n",
      "Processing 1983-01-05 00:00:00\n",
      "Processing 1983-04-15 00:00:00\n",
      "Processing 1983-07-24 00:00:00\n",
      "Processing 1983-11-01 00:00:00\n",
      "Processing 1984-02-09 00:00:00\n",
      "Processing 1984-05-19 00:00:00\n",
      "Processing 1984-08-27 00:00:00\n",
      "Processing 1984-12-05 00:00:00\n",
      "Processing 1985-03-15 00:00:00\n",
      "Processing 1985-06-23 00:00:00\n",
      "Processing 1985-10-01 00:00:00\n",
      "Processing 1986-01-09 00:00:00\n",
      "Processing 1986-04-19 00:00:00\n",
      "Processing 1986-07-28 00:00:00\n",
      "Processing 1986-11-05 00:00:00\n",
      "Processing 1987-02-13 00:00:00\n",
      "Processing 1987-05-24 00:00:00\n",
      "Processing 1987-09-01 00:00:00\n",
      "Processing 1987-12-10 00:00:00\n",
      "Processing 1988-03-19 00:00:00\n",
      "Processing 1988-06-27 00:00:00\n",
      "Processing 1988-10-05 00:00:00\n",
      "Processing 1989-01-13 00:00:00\n",
      "Processing 1989-04-23 00:00:00\n",
      "Processing 1989-08-01 00:00:00\n",
      "Processing 1989-11-09 00:00:00\n",
      "Processing 1990-02-17 00:00:00\n",
      "Processing 1990-05-28 00:00:00\n",
      "Processing 1990-09-05 00:00:00\n",
      "Processing 1990-12-14 00:00:00\n",
      "Processing 1991-03-24 00:00:00\n",
      "Processing 1991-07-02 00:00:00\n",
      "Processing 1991-10-10 00:00:00\n",
      "Processing 1992-01-18 00:00:00\n",
      "Processing 1992-04-27 00:00:00\n",
      "Processing 1992-08-05 00:00:00\n",
      "Processing 1992-11-13 00:00:00\n",
      "Processing 1993-02-21 00:00:00\n",
      "Processing 1993-06-01 00:00:00\n",
      "Processing 1993-09-09 00:00:00\n",
      "Processing 1993-12-18 00:00:00\n",
      "Processing 1994-03-28 00:00:00\n",
      "Processing 1994-07-06 00:00:00\n",
      "Processing 1994-10-14 00:00:00\n",
      "Processing 1995-01-22 00:00:00\n",
      "Processing 1995-05-02 00:00:00\n",
      "Processing 1995-08-10 00:00:00\n",
      "Processing 1995-11-18 00:00:00\n",
      "Processing 1996-02-26 00:00:00\n",
      "Processing 1996-06-05 00:00:00\n",
      "Processing 1996-09-13 00:00:00\n",
      "Processing 1996-12-22 00:00:00\n",
      "Processing 1997-04-01 00:00:00\n",
      "Processing 1997-07-10 00:00:00\n",
      "Processing 1997-10-18 00:00:00\n",
      "Processing 1998-01-26 00:00:00\n",
      "Processing 1998-05-06 00:00:00\n",
      "Processing 1998-08-14 00:00:00\n",
      "Processing 1998-11-22 00:00:00\n",
      "Processing 1999-03-02 00:00:00\n",
      "Processing 1999-06-10 00:00:00\n",
      "Processing 1999-09-18 00:00:00\n",
      "Processing 1999-12-27 00:00:00\n",
      "Processing 2000-04-05 00:00:00\n",
      "Processing 2000-07-14 00:00:00\n",
      "Processing 2000-10-22 00:00:00\n",
      "Processing 2001-01-30 00:00:00\n",
      "Processing 2001-05-10 00:00:00\n",
      "Processing 2001-08-18 00:00:00\n",
      "Processing 2001-11-26 00:00:00\n",
      "Processing 2002-03-06 00:00:00\n",
      "Processing 2002-06-14 00:00:00\n",
      "Processing 2002-09-22 00:00:00\n",
      "Processing 2002-12-31 00:00:00\n",
      "Processing 2003-04-10 00:00:00\n",
      "Processing 2003-07-19 00:00:00\n",
      "Processing 2003-10-27 00:00:00\n",
      "Processing 2004-02-04 00:00:00\n",
      "Processing 2004-05-14 00:00:00\n",
      "Processing 2004-08-22 00:00:00\n",
      "Processing 2004-11-30 00:00:00\n",
      "Processing 2005-03-10 00:00:00\n",
      "Processing 2005-06-18 00:00:00\n",
      "Processing 2005-09-26 00:00:00\n",
      "Processing 2006-01-04 00:00:00\n",
      "Processing 2006-04-14 00:00:00\n",
      "Processing 2006-07-23 00:00:00\n",
      "Processing 2006-10-31 00:00:00\n",
      "Processing 2007-02-08 00:00:00\n",
      "Processing 2007-05-19 00:00:00\n",
      "Processing 2007-08-27 00:00:00\n",
      "Processing 2007-12-05 00:00:00\n",
      "Processing 2008-03-14 00:00:00\n",
      "Processing 2008-06-22 00:00:00\n",
      "Processing 2008-09-30 00:00:00\n",
      "Processing 2009-01-08 00:00:00\n",
      "Processing 2009-04-18 00:00:00\n",
      "Processing 2009-07-27 00:00:00\n",
      "Processing 2009-11-04 00:00:00\n",
      "Processing 2010-02-12 00:00:00\n",
      "Processing 2010-05-23 00:00:00\n",
      "Processing 2010-08-31 00:00:00\n",
      "Processing 2010-12-09 00:00:00\n",
      "Processing 2011-03-19 00:00:00\n",
      "Processing 2011-06-27 00:00:00\n",
      "Processing 2011-10-05 00:00:00\n",
      "Processing 2012-01-13 00:00:00\n",
      "Processing 2012-04-22 00:00:00\n",
      "Processing 2012-07-31 00:00:00\n",
      "Processing 2012-11-08 00:00:00\n",
      "Processing 2013-02-16 00:00:00\n",
      "Processing 2013-05-27 00:00:00\n",
      "Processing 2013-09-04 00:00:00\n",
      "Processing 2013-12-13 00:00:00\n",
      "Processing 2014-03-23 00:00:00\n",
      "Processing 2014-07-01 00:00:00\n",
      "Processing 2014-10-09 00:00:00\n",
      "Processing 2015-01-17 00:00:00\n",
      "Processing 2015-04-27 00:00:00\n",
      "Processing 2015-08-05 00:00:00\n",
      "Processing 2015-11-13 00:00:00\n",
      "Processing 2016-02-21 00:00:00\n",
      "Processing 2016-05-31 00:00:00\n",
      "Processing 2016-09-08 00:00:00\n",
      "Processing 2016-12-17 00:00:00\n",
      "Processing 2017-03-27 00:00:00\n",
      "Processing 2017-07-05 00:00:00\n",
      "Processing 2017-10-13 00:00:00\n",
      "Processing 2018-01-21 00:00:00\n",
      "Processing 2018-05-01 00:00:00\n",
      "Processing 2018-08-09 00:00:00\n",
      "Processing 2018-11-17 00:00:00\n",
      "Processing 2019-02-25 00:00:00\n",
      "Processing 2019-06-05 00:00:00\n",
      "Processing 2019-09-13 00:00:00\n",
      "Processing 2019-12-22 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ---- Read NLDAS daily runoff data & combine to a single file ---- #\n",
    "\n",
    "nldas_runoff_dir = f'{data_dir}/processed/nldas_daily'\n",
    "\n",
    "nc_files_list = [f for f in os.listdir(nldas_runoff_dir) if f.endswith('.nc')]\n",
    "date_list = pd.to_datetime([f.split('.')[0] for f in nc_files_list]).sort_values().tolist()\n",
    "\n",
    "# get basic dimension info from the first nc file\n",
    "with nc.Dataset(f'{nldas_runoff_dir}/{nc_files_list[0]}') as daily_runoff:\n",
    "    lat_array_conus = daily_runoff.variables['lat'][:]\n",
    "    lon_array_conus = daily_runoff.variables['lon'][:]\n",
    "\n",
    "# create a new nc dataset\n",
    "combined_nc_file_path = f'{data_dir}/processed/LRR/input/nldas_runoff.nc'\n",
    "with nc.Dataset(combined_nc_file_path, 'w') as combined_nc:\n",
    "    # create dimensions\n",
    "    combined_nc.createDimension('time', len(date_list))\n",
    "    combined_nc.createDimension('lat', len(lat_array_conus))\n",
    "    combined_nc.createDimension('lon', len(lon_array_conus))\n",
    "\n",
    "    # create variables\n",
    "    combined_nc.createVariable('time', 'S10', ('time',))\n",
    "    combined_nc.createVariable('lat', 'f4', ('lat',))\n",
    "    combined_nc.createVariable('lon', 'f4', ('lon',))\n",
    "    combined_nc.createVariable('Qs', 'f8', ('time', 'lat', 'lon',))\n",
    "    combined_nc.createVariable('Qsb', 'f8', ('time', 'lat', 'lon',))\n",
    "\n",
    "    # variable attributes\n",
    "    combined_nc.variables['time'].units = 'none'\n",
    "    combined_nc.variables['time'].long_name = 'string datetime (yyyy-mm-dd) from 1980-01-01 to 2019-12-31'\n",
    "    combined_nc.variables['lat'].long_name = 'latitude'\n",
    "    combined_nc.variables['lon'].long_name = 'longitude'\n",
    "    combined_nc.variables['Qs'].long_name = 'surface runoff (mm/d)'\n",
    "    combined_nc.variables['Qsb'].long_name = 'baseflow (mm/d)'\n",
    "\n",
    "    # write data\n",
    "    combined_nc.variables['time'][:] = np.array(date_list).astype('datetime64[D]').astype(str)\n",
    "    combined_nc.variables['lat'][:] = lat_array_conus\n",
    "    combined_nc.variables['lon'][:] = lon_array_conus\n",
    "\n",
    "    # write runoff data from each nc file\n",
    "    for i, date in enumerate(date_list):\n",
    "        if i % 100 == 0:\n",
    "            print(f'Processing {date}')\n",
    "        year = date.year\n",
    "        month = f'{date.month:02d}'\n",
    "        day = f'{date.day:02d}'\n",
    "\n",
    "        nc_file_path = f'{nldas_runoff_dir}/{year}{month}{day}.nc'\n",
    "        with nc.Dataset(nc_file_path) as daily_runoff:\n",
    "            # PLEASE NOTE FOR THE RUNOFF DATA\n",
    "            # The runoff data in the original nc files are hourly average values for each day (mm/hr)\n",
    "            # So, here, I need to multiply the values by 24 to get the daily values (mm/day)\n",
    "            combined_nc.variables['Qs'][date_list.index(date), :, :] = daily_runoff.variables['Qs'][:] * 24    # convert from mm/hr to mm/day\n",
    "            combined_nc.variables['Qsb'][date_list.index(date), :, :] = daily_runoff.variables['Qsb'][:] * 24    # convert from mm/hr to mm/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Test the runoff data ---- #\n",
    "\n",
    "with nc.Dataset(combined_nc_file_path) as conus_runoff:\n",
    "    # Qs = conus_runoff.variables['Qs'][:]\n",
    "    # Qsb = conus_runoff.variables['Qsb'][:]\n",
    "\n",
    "    time = conus_runoff.variables['time'][:]\n",
    "\n",
    "start_date = '1980-01-01'\n",
    "end_date = '1980-12-31'\n",
    "start_date_index = np.where(time==start_date)[0][0]\n",
    "end_date_index = np.where(time==end_date)[0][0]\n",
    "\n",
    "time_sub = time[start_date_index:end_date_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1980-01-01', '1980-01-02', '1980-01-03', '1980-01-04',\n",
       "       '1980-01-05', '1980-01-06', '1980-01-07', '1980-01-08',\n",
       "       '1980-01-09', '1980-01-10', '1980-01-11', '1980-01-12',\n",
       "       '1980-01-13', '1980-01-14', '1980-01-15', '1980-01-16',\n",
       "       '1980-01-17', '1980-01-18', '1980-01-19', '1980-01-20',\n",
       "       '1980-01-21', '1980-01-22', '1980-01-23', '1980-01-24',\n",
       "       '1980-01-25', '1980-01-26', '1980-01-27', '1980-01-28',\n",
       "       '1980-01-29', '1980-01-30', '1980-01-31', '1980-02-01',\n",
       "       '1980-02-02', '1980-02-03', '1980-02-04', '1980-02-05',\n",
       "       '1980-02-06', '1980-02-07', '1980-02-08', '1980-02-09',\n",
       "       '1980-02-10', '1980-02-11', '1980-02-12', '1980-02-13',\n",
       "       '1980-02-14', '1980-02-15', '1980-02-16', '1980-02-17',\n",
       "       '1980-02-18', '1980-02-19', '1980-02-20', '1980-02-21',\n",
       "       '1980-02-22', '1980-02-23', '1980-02-24', '1980-02-25',\n",
       "       '1980-02-26', '1980-02-27', '1980-02-28', '1980-02-29',\n",
       "       '1980-03-01', '1980-03-02', '1980-03-03', '1980-03-04',\n",
       "       '1980-03-05', '1980-03-06', '1980-03-07', '1980-03-08',\n",
       "       '1980-03-09', '1980-03-10', '1980-03-11', '1980-03-12',\n",
       "       '1980-03-13', '1980-03-14', '1980-03-15', '1980-03-16',\n",
       "       '1980-03-17', '1980-03-18', '1980-03-19', '1980-03-20',\n",
       "       '1980-03-21', '1980-03-22', '1980-03-23', '1980-03-24',\n",
       "       '1980-03-25', '1980-03-26', '1980-03-27', '1980-03-28',\n",
       "       '1980-03-29', '1980-03-30', '1980-03-31', '1980-04-01',\n",
       "       '1980-04-02', '1980-04-03', '1980-04-04', '1980-04-05',\n",
       "       '1980-04-06', '1980-04-07', '1980-04-08', '1980-04-09',\n",
       "       '1980-04-10', '1980-04-11', '1980-04-12', '1980-04-13',\n",
       "       '1980-04-14', '1980-04-15', '1980-04-16', '1980-04-17',\n",
       "       '1980-04-18', '1980-04-19', '1980-04-20', '1980-04-21',\n",
       "       '1980-04-22', '1980-04-23', '1980-04-24', '1980-04-25',\n",
       "       '1980-04-26', '1980-04-27', '1980-04-28', '1980-04-29',\n",
       "       '1980-04-30', '1980-05-01', '1980-05-02', '1980-05-03',\n",
       "       '1980-05-04', '1980-05-05', '1980-05-06', '1980-05-07',\n",
       "       '1980-05-08', '1980-05-09', '1980-05-10', '1980-05-11',\n",
       "       '1980-05-12', '1980-05-13', '1980-05-14', '1980-05-15',\n",
       "       '1980-05-16', '1980-05-17', '1980-05-18', '1980-05-19',\n",
       "       '1980-05-20', '1980-05-21', '1980-05-22', '1980-05-23',\n",
       "       '1980-05-24', '1980-05-25', '1980-05-26', '1980-05-27',\n",
       "       '1980-05-28', '1980-05-29', '1980-05-30', '1980-05-31',\n",
       "       '1980-06-01', '1980-06-02', '1980-06-03', '1980-06-04',\n",
       "       '1980-06-05', '1980-06-06', '1980-06-07', '1980-06-08',\n",
       "       '1980-06-09', '1980-06-10', '1980-06-11', '1980-06-12',\n",
       "       '1980-06-13', '1980-06-14', '1980-06-15', '1980-06-16',\n",
       "       '1980-06-17', '1980-06-18', '1980-06-19', '1980-06-20',\n",
       "       '1980-06-21', '1980-06-22', '1980-06-23', '1980-06-24',\n",
       "       '1980-06-25', '1980-06-26', '1980-06-27', '1980-06-28',\n",
       "       '1980-06-29', '1980-06-30', '1980-07-01', '1980-07-02',\n",
       "       '1980-07-03', '1980-07-04', '1980-07-05', '1980-07-06',\n",
       "       '1980-07-07', '1980-07-08', '1980-07-09', '1980-07-10',\n",
       "       '1980-07-11', '1980-07-12', '1980-07-13', '1980-07-14',\n",
       "       '1980-07-15', '1980-07-16', '1980-07-17', '1980-07-18',\n",
       "       '1980-07-19', '1980-07-20', '1980-07-21', '1980-07-22',\n",
       "       '1980-07-23', '1980-07-24', '1980-07-25', '1980-07-26',\n",
       "       '1980-07-27', '1980-07-28', '1980-07-29', '1980-07-30',\n",
       "       '1980-07-31', '1980-08-01', '1980-08-02', '1980-08-03',\n",
       "       '1980-08-04', '1980-08-05', '1980-08-06', '1980-08-07',\n",
       "       '1980-08-08', '1980-08-09', '1980-08-10', '1980-08-11',\n",
       "       '1980-08-12', '1980-08-13', '1980-08-14', '1980-08-15',\n",
       "       '1980-08-16', '1980-08-17', '1980-08-18', '1980-08-19',\n",
       "       '1980-08-20', '1980-08-21', '1980-08-22', '1980-08-23',\n",
       "       '1980-08-24', '1980-08-25', '1980-08-26', '1980-08-27',\n",
       "       '1980-08-28', '1980-08-29', '1980-08-30', '1980-08-31',\n",
       "       '1980-09-01', '1980-09-02', '1980-09-03', '1980-09-04',\n",
       "       '1980-09-05', '1980-09-06', '1980-09-07', '1980-09-08',\n",
       "       '1980-09-09', '1980-09-10', '1980-09-11', '1980-09-12',\n",
       "       '1980-09-13', '1980-09-14', '1980-09-15', '1980-09-16',\n",
       "       '1980-09-17', '1980-09-18', '1980-09-19', '1980-09-20',\n",
       "       '1980-09-21', '1980-09-22', '1980-09-23', '1980-09-24',\n",
       "       '1980-09-25', '1980-09-26', '1980-09-27', '1980-09-28',\n",
       "       '1980-09-29', '1980-09-30', '1980-10-01', '1980-10-02',\n",
       "       '1980-10-03', '1980-10-04', '1980-10-05', '1980-10-06',\n",
       "       '1980-10-07', '1980-10-08', '1980-10-09', '1980-10-10',\n",
       "       '1980-10-11', '1980-10-12', '1980-10-13', '1980-10-14',\n",
       "       '1980-10-15', '1980-10-16', '1980-10-17', '1980-10-18',\n",
       "       '1980-10-19', '1980-10-20', '1980-10-21', '1980-10-22',\n",
       "       '1980-10-23', '1980-10-24', '1980-10-25', '1980-10-26',\n",
       "       '1980-10-27', '1980-10-28', '1980-10-29', '1980-10-30',\n",
       "       '1980-10-31', '1980-11-01', '1980-11-02', '1980-11-03',\n",
       "       '1980-11-04', '1980-11-05', '1980-11-06', '1980-11-07',\n",
       "       '1980-11-08', '1980-11-09', '1980-11-10', '1980-11-11',\n",
       "       '1980-11-12', '1980-11-13', '1980-11-14', '1980-11-15',\n",
       "       '1980-11-16', '1980-11-17', '1980-11-18', '1980-11-19',\n",
       "       '1980-11-20', '1980-11-21', '1980-11-22', '1980-11-23',\n",
       "       '1980-11-24', '1980-11-25', '1980-11-26', '1980-11-27',\n",
       "       '1980-11-28', '1980-11-29', '1980-11-30', '1980-12-01',\n",
       "       '1980-12-02', '1980-12-03', '1980-12-04', '1980-12-05',\n",
       "       '1980-12-06', '1980-12-07', '1980-12-08', '1980-12-09',\n",
       "       '1980-12-10', '1980-12-11', '1980-12-12', '1980-12-13',\n",
       "       '1980-12-14', '1980-12-15', '1980-12-16', '1980-12-17',\n",
       "       '1980-12-18', '1980-12-19', '1980-12-20', '1980-12-21',\n",
       "       '1980-12-22', '1980-12-23', '1980-12-24', '1980-12-25',\n",
       "       '1980-12-26', '1980-12-27', '1980-12-28', '1980-12-29',\n",
       "       '1980-12-30', '1980-12-31'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
